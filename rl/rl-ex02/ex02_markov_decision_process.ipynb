{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7836d1-40cf-4da9-80d1-f8dc0146382f",
   "metadata": {},
   "source": [
    "# Exercise 02: Markov Decision Processes\n",
    "\n",
    "In this exercise, we want to look at markov decision processes (MDP) and how to model and use them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87df1abb-2aeb-492c-89f9-75e1c3e4921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a1fc-be3f-4fb2-89ea-0e101e730efe",
   "metadata": {},
   "source": [
    "A MDP is modelled by states and transition between states through the function\n",
    "\n",
    "$$\\def\\Pr#1{{{\\mathrm{Pr}\\!}\\left\\{#1\\right\\}}}p(s',r|s,a) \\doteq \\Pr{S_t\\!=\\!s', R_t\\!=\\!r\\;|\\;S_{t-1}\\!=\\!s, A_{t-1}\\!=\\!a}$$\n",
    "\n",
    "We will model this in a class for a state that contains the list of transitions for each actions. Each transition contains the next state and the corresponding reward.\n",
    "\n",
    "Please study the definition of the class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3e3f58-6de6-465a-b64f-a325a9383bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RState:\n",
    "    \"\"\"\n",
    "    State in a stochastic MDP. Each action can result in different states and rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_id):\n",
    "        self._state_id = state_id\n",
    "\n",
    "        # transition is a dict with an entry for each action which in turn is a list of transitions\n",
    "        # each transition is a dict with probability, next_state and reward\n",
    "        self._transitions = {}\n",
    "        self._is_terminal = False\n",
    "\n",
    "    @property\n",
    "    def state_id(self):\n",
    "        return self._state_id\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self._is_terminal\n",
    "\n",
    "    @is_terminal.setter\n",
    "    def is_terminal(self, value):\n",
    "        self._is_terminal = value\n",
    "\n",
    "    def add_transition(self, action, next_state, reward, probability):\n",
    "        \"\"\"\n",
    "        Add a transition to the state.\n",
    "        Args:\n",
    "            action: action for which the transition is added\n",
    "            next_state: the next state after taking the action\n",
    "            reward: the reward for taking the action\n",
    "            probability: the probability of the transition\n",
    "        \"\"\"\n",
    "        if action not in self._transitions:\n",
    "            self._transitions[action] = []\n",
    "        self._transitions[action].append({'probability': probability, \n",
    "                                          'next_state': next_state,\n",
    "                                          'reward': reward})\n",
    "\n",
    "    def add_transition_rescale(self, action, next_state, reward, probability):\n",
    "        \"\"\"\n",
    "        Add a transition to the state with the give probability and rescale all other transition probabilities\n",
    "        so that the probabilities sum to 1.0.\n",
    "\n",
    "        Args:\n",
    "            action: action for which the transition is added\n",
    "            next_state: the next state after taking the action\n",
    "            reward: the reward for taking the action\n",
    "            probability: the probability of the transition\n",
    "        \"\"\"\n",
    "        if action not in self._transitions:\n",
    "            self._transitions[action] = []\n",
    "\n",
    "        # check if the previous transitions sum to 1.0, if not they need to be rescaled for that too\n",
    "        sum_prob = 0.0\n",
    "        for transition in self._transitions[action]:\n",
    "            sum_prob += transition['probability']\n",
    "        scale = 1.0 / sum_prob\n",
    "        scale *= (1.0 - probability)\n",
    "        transitions = self._transitions[action]\n",
    "        for i in range(len(transitions)):\n",
    "            transitions[i]['probability'] *= scale\n",
    "        self._transitions[action].append({'probability': probability, \n",
    "                                          'next_state': next_state,\n",
    "                                          'reward': reward})\n",
    "\n",
    "    def take_action(self, action) -> (int, float):\n",
    "        \"\"\"\n",
    "            Take an action in the state to get the next state and reward\n",
    "        Args:\n",
    "            action: the action to take\n",
    "\n",
    "        Returns:\n",
    "            the next state and reward\n",
    "        \"\"\"\n",
    "        if self._is_terminal:\n",
    "            raise Exception(\"Action on terminal state\")\n",
    "\n",
    "        # select a transition according to the probabilities\n",
    "        transitions = self._transitions[action]\n",
    "        probabilities = [t['probability'] for t in transitions]\n",
    "        index = np.random.choice(len(transitions), p=probabilities)\n",
    "        return transitions[index]['next_state'], transitions[index]['reward'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7f4a6-7bb9-46d3-a746-67ebca436860",
   "metadata": {},
   "source": [
    "## Defining an MDP\n",
    "\n",
    "Using the class above, we would like to model the MDP given by the following diagram\n",
    "\n",
    "<div>\n",
    "<img src=\"MDP.png\" width=\"40%\"/>\n",
    "</div>\n",
    "\n",
    "where s2 is a terminal state. \n",
    "So for example for action $a_0$ in $S_0$, there is a 70% change to reach state $S_1$ and a 30% change to stay in state $S_0$.\n",
    "\n",
    "\n",
    "Add the necessary commands to create that MDP. All actions give a reward of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a48ef8-a3a1-4985-bec8-737e5e2cbdb6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1c806d47b6f2b65985d712bbd77430a",
     "grade": false,
     "grade_id": "cell-cb422877ccf459ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "REWARD = -1\n",
    "\n",
    "# actions\n",
    "a0 = 0\n",
    "a1 = 1\n",
    "\n",
    "# states\n",
    "s0 = RState(0)\n",
    "s1 = RState(1)\n",
    "s2 = RState(2)\n",
    "\n",
    "# S0\n",
    "s0.add_transition(a0, s0, REWARD, 0.3)\n",
    "s0.add_transition(a0, s1, REWARD, 0.7)\n",
    "s0.add_transition(a1, s0, REWARD, 0.5)\n",
    "s0.add_transition(a1, s2, REWARD, 0.5)\n",
    "\n",
    "# S1\n",
    "s1.add_transition(a0, s1, REWARD, 0.2)\n",
    "s1.add_transition(a0, s2, REWARD, 0.8)\n",
    "\n",
    "s1.add_transition(a1, s1, REWARD, 0.4)\n",
    "s1.add_transition(a1, s0, REWARD, 0.6)\n",
    "\n",
    "# S2\n",
    "s2.is_terminal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d10d91-1aa6-4a4b-a2fd-f051bb1972ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5b7fb890c986e8e27160c54da818baa",
     "grade": true,
     "grade_id": "cell-ebb6bb5026ce4808",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert s2.is_terminal\n",
    "assert not s0.is_terminal\n",
    "assert not s1.is_terminal\n",
    "\n",
    "assert len(s0._transitions) == 2\n",
    "assert len(s0._transitions[a0]) == 2\n",
    "\n",
    "t0 = s0._transitions[a0][0]\n",
    "t1 = s0._transitions[a0][1]\n",
    "assert t0['probability'] == 0.3 or t1['probability']==0.7\n",
    "assert t0['reward'] == -1\n",
    "assert t0['next_state'] == s1 or t1['next_state'] == s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ad896-56cc-4c1d-bef6-ac13861b55ee",
   "metadata": {},
   "source": [
    "## Policy function\n",
    "\n",
    "Now we want to define a policy for this MDP. A policy defines a probability for each action in each state. We want to start with a uniformely random policy, so that each action has the same probability.\n",
    "\n",
    "\n",
    "Define an appropriate matrix using numpy. As $S_2$ is a terminal state, there can never be an action taken in this states. The only states with actions are $S_0$ and $S_1$, and the possible actions for both states are $A_0$ and $A_1$. Use the state as the first index in the matrix and the action as the second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0339ec-1597-47dc-b6f8-9ef0e542127d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff91922a7a6b30f387dcfd19ee64d389",
     "grade": false,
     "grade_id": "cell-01dbf913bea5070d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "policy = np.full((2, 2), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6aa3c42-3658-48f1-b5f1-e50e04f03b58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04c8c9820e51d8e4a111a409d5f3ecc1",
     "grade": true,
     "grade_id": "cell-432e0f716e752b3b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert policy.shape == (2,2)\n",
    "assert policy[0,1] == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9bd23-8476-4de4-aece-9c0abc8fbb89",
   "metadata": {},
   "source": [
    "## Episodes\n",
    "\n",
    "In the next few lectures we will encounter episodes. An episode is a sequence of states (and rewards) until a terminal state is reached. \n",
    "\n",
    "We want to calculate different episodes and print the results.\n",
    "\n",
    "Complete the function sample_episode below. It should return a sequence of state ids and the return (sum of the rewards) of the episode. We will not use reward discounting in this example. The possible actions are always numbered 0, 1 etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6a323ba-88d5-4f08-9228-836370df55ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5487309a240a4aae041a26ec31383b6",
     "grade": false,
     "grade_id": "cell-2745c50a8798711d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_episode(start: RState, policy) -> ([], float):\n",
    "    s = start\n",
    "    r = 0\n",
    "    nr_actions = policy.shape[1]\n",
    "\n",
    "    # the trajectory is a list of [state_id, action, reward] lists for each state taken\n",
    "    trajectory = []\n",
    "\n",
    "    while not s.is_terminal:\n",
    "        # Sample an action according to the policy for the current state\n",
    "        action_probs = policy[s.state_id]\n",
    "        action = np.random.choice(nr_actions, p=action_probs)\n",
    "        \n",
    "        # Take the action to get next state and reward\n",
    "        next_state, reward = s.take_action(action)\n",
    "        \n",
    "        # Record this step in the trajectory\n",
    "        trajectory.append([s.state_id, action, reward])\n",
    "        \n",
    "        # Accumulate the return (sum of rewards)\n",
    "        r += reward\n",
    "        \n",
    "        # Move to the next state\n",
    "        s = next_state\n",
    "    \n",
    "    return trajectory, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753c76c5-62d6-4513-a26a-21a436c379e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b19bfb140be57429668fd1d90327d446",
     "grade": true,
     "grade_id": "cell-c98f5b759b42a980",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, -1], [1, 0, -1]] -2\n",
      "-3.49\n"
     ]
    }
   ],
   "source": [
    "t, r = sample_episode(s0, policy)\n",
    "print(t, r)\n",
    "assert len(t) >= 1\n",
    "assert r <= -1.0\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    t, r = sample_episode(s0, policy)\n",
    "    results.append(r)\n",
    "\n",
    "r_array = np.array(results)\n",
    "mean = np.mean(r_array)\n",
    "print(mean)\n",
    "\n",
    "assert mean < -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bc562-c05a-4449-a91c-55640210ef51",
   "metadata": {},
   "source": [
    "A random policy is certainly not optimal, so we would like to find a optimal one. But as we do yet know how to calculate an optimal policy, we will have to guess.\n",
    "\n",
    "There are 2 possiblities:\n",
    "* We always take action $A_1$ in $S_0$, and either reach $S_2$ immediately with 50% chance, or stay in $S_0$ and try again.\n",
    "* We take action $A_0$ in $S_0$ and again in $S_1$. This gives as a higher chance to reach the next state, but it takes 2 steps. We never want to take action $A_1$ in $S_1$ as this would put us back to the start state.\n",
    "\n",
    "\n",
    "Implement both these policies and calculate the mean return over 100 episodes? Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992fa094-5130-4fed-bfee-9fe15aae0e20",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36421ea867d9006dd0b1174bc0d22ac6",
     "grade": false,
     "grade_id": "cell-a969dd6916329449",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Change the policy, so that it is optimal\n",
    "policy_0 = np.zeros((2, 2))\n",
    "policy_1 = np.zeros((2, 2))\n",
    "\n",
    "policy_0[0][1] = 1\n",
    "\n",
    "policy_1[0][0] = 1\n",
    "policy_1[1][0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d39265e-1e98-4123-a553-dd1c34aa0037",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d301a7c056a2eb104cf37121971ae0c",
     "grade": true,
     "grade_id": "cell-b4276fe2c3f27199",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.86 -2.63\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(policy, start):\n",
    "    results = []\n",
    "    for i in range(100):\n",
    "        t, r = sample_episode(start, policy)\n",
    "        results.append(r)\n",
    "    r_array = np.array(results)\n",
    "    return np.mean(r_array)\n",
    "\n",
    "\n",
    "mean_0 = evaluate_policy(policy_0, s0)\n",
    "mean_1 = evaluate_policy(policy_1, s0)\n",
    "\n",
    "assert mean_0 < -1.0\n",
    "assert mean_1 < -1.0\n",
    "print(mean_0, mean_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbf81b-f651-4da1-8f90-13922c253626",
   "metadata": {},
   "source": [
    "It seems that the first policy is better.\n",
    "\n",
    "In the lecture, we have seen that we can solve a MDP using a system of equations. If we only look at state $S_0$, can you write and solve the equation for that state?\n",
    "\n",
    "What is the value function for $S_0$? Type your value in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b68e9-9e5e-4cfb-b423-9569b25ce6d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86cc402c32f750fbac7557b2a0170e70",
     "grade": true,
     "grade_id": "cell-3ac3a3c423239633",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75fab0-ad15-4dd8-b904-bd43e6e699fb",
   "metadata": {},
   "source": [
    "## Gridworld environment\n",
    "\n",
    "We would like to use a gridworld environment in some of the exercises. Using the state class above, the following class definition defines a gridworld where an action in each direction (N, E, S, W) takes you to the neighboring cell except on the border where you stay in the same position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34f9652-7d13-4833-a8ff-0a4b37ff14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPGridworld:\n",
    "    \"\"\"\n",
    "    MDP describing a gridworld.\n",
    "\n",
    "    - States are (logically) distributed in a grid of size width x height\n",
    "    - States are kept in 1D array by their state id\n",
    "    - Actions are up, down, left, right (N, S, W, E)\n",
    "    - Actions take you to the neighboring state per default, unless you are at the border\n",
    "    - Action behaviour can be changed to be stochastic, or to transition to a different state\n",
    "\n",
    "    - The origin of the coordinate system is in the upper left corner with x0 going down and x1 going right\n",
    "    - This corresponds to the matrix indexing in numpy\n",
    "\n",
    "    \"\"\"\n",
    "    N = 0\n",
    "    E = 1\n",
    "    S = 2\n",
    "    W = 3\n",
    "    NR_ACTIONS = 4\n",
    "\n",
    "    def __init__(self, height, width):\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "        self._states = np.empty((height * width), dtype=object)\n",
    "\n",
    "        # create the states first\n",
    "        for x0 in range(self._height):\n",
    "            for x1 in range(self._width):\n",
    "                state_id = self.pos_to_id(x0, x1)\n",
    "                s = RState(state_id)\n",
    "                self._states[state_id] = s\n",
    "\n",
    "        # create the transitions\n",
    "        for x0 in range(self._height):\n",
    "            for x1 in range(self._width):\n",
    "                state_id = self.pos_to_id(x0, x1)\n",
    "                s = self._states[state_id]\n",
    "\n",
    "                # make a default grid with 4 actions for each state and connections to the neighbors\n",
    "                # North\n",
    "                if x0 != 0:\n",
    "                    s.add_transition(self.N, self._states[self.pos_to_id(x0 - 1, x1)], -1.0, 1.0)\n",
    "                else:\n",
    "                    s.add_transition(self.N, self._states[self.pos_to_id(x0, x1)], -1.0, 1.0)\n",
    "\n",
    "                # South\n",
    "                if x0 != self._height - 1:\n",
    "                    s.add_transition(self.S, self._states[self.pos_to_id(x0 + 1, x1)], -1.0, 1.0)\n",
    "                else:\n",
    "                    s.add_transition(self.S, self._states[self.pos_to_id(x0, x1)], -1.0, 1.0)\n",
    "                # West\n",
    "                if x1 != 0:\n",
    "                    s.add_transition(self.W, self._states[self.pos_to_id(x0, x1 - 1)], -1.0, 1.0)\n",
    "                else:\n",
    "                    s.add_transition(self.W, self._states[self.pos_to_id(x0, x1)], -1.0, 1.0)\n",
    "\n",
    "                # East\n",
    "                if x1 != self._width - 1:\n",
    "                    s.add_transition(self.E, self._states[self.pos_to_id(x0, x1 + 1)], -1.0, 1.0)\n",
    "                else:\n",
    "                    s.add_transition(self.E, self._states[self.pos_to_id(x0, x1)], -1.0, 1.0)\n",
    "\n",
    "    def state(self, x0, x1):\n",
    "        return self._states[self.pos_to_id(x0, x1)]\n",
    "\n",
    "    def pos_to_id(self, x0, x1) -> int:\n",
    "        return x1 * self._height + x0\n",
    "\n",
    "    def id_to_pos(self, id) -> (int, int):\n",
    "        return id % self._height, id // self._height\n",
    "\n",
    "    def all_state_ids(self):\n",
    "        ids = np.zeros((self._height, self._width), dtype=int)\n",
    "        for x0 in range(self._height):\n",
    "            for x1 in range(self._width):\n",
    "                ids[x0, x1] = self.pos_to_id(x0, x1)\n",
    "        return ids\n",
    "\n",
    "    def take_action(self, x0, x1, action) -> (int, int, float):\n",
    "        \"\"\"\n",
    "            Take an action in the grid to get the next position and reward.\n",
    "\n",
    "            Actions can also be performed directly using the underlying state objects. This method is just a\n",
    "            convenience when you want to use the positions instead.\n",
    "        Args:\n",
    "            x0: x0 coordinate of the state\n",
    "            x1: x1 coordinate of the state\n",
    "            action: the action to take\n",
    "\n",
    "        Returns:\n",
    "            the next state and reward\n",
    "        \"\"\"\n",
    "        state, reward = self._states[self.pos_to_id(x0, x1)].take_action(action)\n",
    "        x0, x1 = self.id_to_pos(state.state_id)\n",
    "        return x0, x1, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f712f774-804f-427f-9719-f13b144019d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = MDPGridworld(3, 4)\n",
    "g.state(0,0).is_terminal = True\n",
    "for i in range(12):\n",
    "    assert g.pos_to_id(g.id_to_pos(i)[0],g.id_to_pos(i)[1]) == i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f82cc34f-ce6a-4989-8779-c46c48ab72a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAGiCAYAAABd3URpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHI9JREFUeJzt3Xuc1XWdx/HPyGVgYRhFAyQGIVBEEFIgHUTygpN4eUiZaRqCYd4QQ9JVxDU1dVDL2xoEmdhlTWpLZbuYiAkiUoCgrppKsTIoiLo6A6SDDGf/aGWXSHT4zjm/gXk+H4958DhnDo/f+8FP5rzObw5OUS6XywUAwA7aLesBAMDOTUwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnERAFMmTIlunfvHq1atYoBAwbE448/nvUk8mDevHlx4oknRufOnaOoqCgeeOCBrCeRB5WVlTFo0KAoKSmJDh06xIgRI+LFF1/MehZ5MHXq1OjXr1+0a9cu2rVrF+Xl5fHb3/4261mNkpjIs5kzZ8b48eNj0qRJsXTp0jj88MNj+PDhsXLlyqyn0cA2bNgQ/fv3jzvvvDPrKeTR3LlzY+zYsbFw4cKYPXt2bNq0KSoqKmLDhg1ZT6OBdenSJSZPnhyLFy+OxYsXx1FHHRUnnXRSPPfcc1lPa3SK/KCv/DrkkEPi4IMPjqlTp265r3fv3jFixIiorKzMcBn5VFRUFPfff3+MGDEi6ynk2RtvvBEdOnSIuXPnxtChQ7OeQ561b98+br755hgzZkzWUxoVVybyaOPGjbFkyZKoqKjY6v6KiopYsGBBRquAhlRdXR0Rf3uSYddVV1cX9913X2zYsCHKy8uzntPoNM96wK7szTffjLq6uujYseNW93fs2DHWrFmT0SqgoeRyuZgwYUIMGTIk+vbtm/Uc8uDZZ5+N8vLyeO+996Jt27Zx//33xwEHHJD1rEZHTBRAUVHRVrdzudw29wE7nwsvvDCeeeaZmD9/ftZTyJNevXrFsmXL4p133olf/OIXMWrUqJg7d66g+DtiIo/22muvaNas2TZXIdauXbvN1Qpg5zJu3LiYNWtWzJs3L7p06ZL1HPKkZcuW0bNnz4iIGDhwYCxatChuv/32mDZtWsbLGhfvmcijli1bxoABA2L27Nlb3T979uwYPHhwRquAFLlcLi688ML45S9/GY8++mh0794960kUUC6Xi9ra2qxnNDquTOTZhAkTYuTIkTFw4MAoLy+P6dOnx8qVK+O8887LehoNbP369bF8+fItt1esWBHLli2L9u3bR9euXTNcRkMaO3Zs3HvvvfHggw9GSUnJliuPpaWl0bp164zX0ZCuuOKKGD58eJSVlcW6devivvvui8ceeyweeuihrKc1Ov5paAFMmTIlbrrppli9enX07ds3br31Vv+EbBf02GOPxZFHHrnN/aNGjYp77rmn8IPIiw97v9OMGTNi9OjRhR1DXo0ZMybmzJkTq1evjtLS0ujXr19cdtllccwxx2Q9rdEREwBAEu+ZAACSiAkAIImYAACSiAkAIImYAACSiAkAIImYAACSiIkCqK2tjauvvtr/grUJcK6bFue76XCut8//tKoAampqorS0NKqrq6Ndu3ZZzyGPnOumxfluOpzr7XNlAgBIIiYAgCQF/6mhmzdvjtdeey1KSko+9Afm7Gpqamq2+pVdl3PdtDjfTUdTPNe5XC7WrVsXnTt3jt122/61h4K/Z2LVqlVRVlZWyEMCADuoqqoqunTpst3HFPzKRElJSUREfPvW26N/v08X+vAU0JxHHo7JldfH5RMnxdHDKrKeQx45102Hc910PP3Msrjk4q9ved7enoLHxAff2ujf79MxZOjQQh+eAlq1qioiInrt39u53sU5102Hc930fJy3JHgDJgCQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnEBACQREwAAEnERAFMmzol9t+3e+zetlUM/syAmD//8awn0cCmf29qDDqoX3Ro3y46tG8Xnx1SHr976LdZzyJPXn311TjrzK/EJzvuGe3b/VMcMuDT8dSSJVnPIg/WrVsXl0wYH/v12Cf2KGkdRxw+OBYvWpT1rEZHTOTZz382My79xvi47PJJsXDR0hg85PAYccLwWLlyZdbTaECf7NIlvnXD5Hhi4eJ4YuHiOOLIo+KUL5wUzz/3XNbTaGBvv/12HPXZw6JFixbxwH/8NpY+83xMvvk7sfvuu2c9jTw4/9yz49E5s+Pue34ci5c+G8OOqYjjjx0Wr776atbTGpUdiokpU6ZE9+7do1WrVjFgwIB4/HGvtD/MHbfdEqPPGhNnjTk79u/dO759y23Rpawsvj9tatbTaEDHn3BiHDv8uNh3v/1i3/32i2u+dX20bds2/viHhVlPo4F95+Ybo0uXspj+gxkx6DOfiX26dYsjjzo6PtWjR9bTaGDvvvtuPPDLX8T1lTfFkMOHRo+ePePKq66Obt26+xr+d+odEzNnzozx48fHpEmTYunSpXH44YfH8OFeaf8jGzdujKVPLYmjj6nY6v6jh1XEwicXZLSKfKurq4ufzbwvNmzYEIccWp71HBrYr381Kw4eMDBOP+2U6Nq5Qxw68KC4+67vZz2LPNi0aVPU1dVFq1attrq/VevWseCJ+RmtapzqHRO33HJLjBkzJs4+++zo3bt33HbbbVFWVhZTp6q0v/fmm29GXV1ddOjQcav7O3bsGK+/viajVeTLfz77bOy1e9sobVMcF409L2b++/3R+4ADsp5FA1vxl7/E96dNjZ49941Zv/5dnH3OefGNiy+Kf/vxj7KeRgMrKSmJQw4tj8rrvxWvvfZa1NXVxU//7Sex6I9/iDVrVmc9r1GpV0xs3LgxlixZEhUVW7/SrqioiAUL/vEr7dra2qipqdnqo6kpKira6nYul9vmPnZ++/XqFX9YvCzmzl8YXzv3/PjaV0fFC88/n/UsGtjmzZvj0wcdHNded0N8+qCD4uxzzo2zxnwtprvsvUu6+54fRy6Xix77fDJK2xTHd++8I0497fRo1qxZ1tMalXrFxAevtDt23PaV9po1//iVdmVlZZSWlm75KCsr2/G1O5m99tormjVrts1ViLVr125ztYKdX8uWLaNHz54xYODA+Nb1lXFgv/7x3X+9PetZNLBOe+8dvXtvfcVp//17R1WVb/Xuij7Vo0fMfnRuvPnO+nh5RVXMf/KP8f6m96Nbt+5ZT2tUdugNmPV5pT1x4sSorq7e8lFVVbUjh9wptWzZMg46eEA8+sjsre5/dM7sOLR8cEarKJRcLhe1tbVZz6CBlQ8+LF566cWt7nv55Zeia9d9MlpEIbRp0yb23nvvePvtt+ORh38XJ5x4UtaTGpXm9XnwB6+0//4qxNq1a7e5WvGB4uLiKC4u3vGFO7mLxk+IMaNHxsEDBsYhh5bHD+6aHlUrV8bZ55yX9TQa0FVXXhEVxw6Psi5lsW7duvj5z+6LeXMfi1m/fijraTSwcRddHEcOHRw3Tb4hTv7il2LRoj/G3XdNjzunTs96Gnkw++HfRS6Xi/326xV//vPyuOKyS2Pf/XrFmaPPynpao1KvmGjZsmUMGDAgZs+eHZ///Oe33D979uw46SSV9o+c8qVT47/feituuP7aWLN6dfTp0zce+I/fxD77eBWzK1n7+usxZvTIWLN6dZSWlkbfA/vFrF8/FEcPOybraTSwgYMGxcx/vz+umjQxbrju2ujWvXvc/J3b4sunn5H1NPKguro6rrpyYry6alW0b98+Tvr8yXHNt66PFi1aZD2tUalXTERETJgwIUaOHBkDBw6M8vLymD59eqxcuTLOO88r7Q9z7vkXxLnnX5D1DPLoe9//QdYTKKDjjj8hjjv+hKxnUABfPOVL8cVTvpT1jEav3jFx6qmnxltvvRXXXnttrF69Ovr27Ru/+Y1X2gDQVNU7JiIiLrjggrjgAq+0AQA/mwMASCQmAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkYgIASCImAIAkzbM68JxHHo5Vq6qyOjwF8OSCJ7b6lV2Xc910ONdNx4t/euFjP7Yol8vl8rhlGzU1NVFaWlrIQwIAO6i6ujratWu33cdkdmXi8omTotf+vbM6PAXw5IInYvq0qXHOuedH+eDDsp5DHjnXTYdz3XS8+KcXYnLl9R/rsZnFxNHDKmLI0KFZHZ4CmT5tapQPPixOO/2MrKeQZ8510+FcNw3z58372DHhDZgAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxkUfzH58XJ484Mbp37RytWxTFrAcfyHoSBXLzjZXRukVRXDJhfNZTaGC9enaL1i2KtvkYP25s1tNI9FFfs3O5XFx37dXRvWvn2KOkdVQcfUQ8/9xzmWxtbOodE/PmzYsTTzwxOnfuHEVFRfHAAw/kYdauYcOGDXFgv/5x6+13Zj2FAlq8aFH84K7pceCB/bKeQh7Mf3JRrKhaveXj1w/NjoiIL3zxlIyXkeqjvmZ/59s3xR233RK33n5nzH9yUXTs1CmOH35MrFu3rsBLG5/m9f0NGzZsiP79+8dZZ50VJ598cj427TI+d+zw+Nyxw7OeQQGtX78+zhp1Rkz53vdj8g3XZT2HPPjEJz6x1e1v3zQ5PtWjRxw+9LMZLaKhbO9rdi6Xi+/ecVv888RJMeLzX4iIiLvu/mHs88mOMfOn98bZ55xbyKmNTr2vTAwfPjyuu+66+MIXvpCPPbBTGz9ubBw7/Pg46uhhWU+hADZu3Bj33fuTGDX6q1FUVJT1HPLov1asiDVr1sSwYRVb7isuLo7Dh342Fj65IMNljUO9r0zUV21tbdTW1m65XVNTk+9DQiZ+NvO+WLb0qZi/cFHWUyiQWQ8+EO+880585czRWU8hz9asWRMRER06dtzq/g4dOsbKla9kMalRyfsbMCsrK6O0tHTLR1lZWb4PCQVXVVUVl074etz9w59Eq1atsp5Dgfxwxg/ic8cOj86dO2c9hQL5+ytQuVzOVakoQExMnDgxqqurt3xUVVXl+5BQcEufWhJr166NwYcMiLatmkfbVs3j8XlzY8qdd0TbVs2jrq4u64k0sFdeeSUenfNIjP7q2VlPoQA6deoUERGv/+8Vig+88cba6NCh4z/6LU1K3r/NUVxcHMXFxfk+DGTqyKOOjsVLn93qvnPOPit69do/vnHpZdGsWbOMlpEvP/7hjOjQoUMMP+74rKdQAN26d49OnTrFnDmz49MHHRQRf3vPzOPz5sZ1N9yY8brs5T0mmrL169fHn5cv33L7v1asiKeXLYs92rePrl27ZriMhlZSUhJ9+vbd6r42bdpE+z333OZ+dn6bN2+OH/1wRpwxclQ0b+7L6K7io75mj71ofNw8+Ybo2XPf6Nlz37jpxhui9T/9U5z65dMzXN041Ptvwfr162P5//vDXrFiRSxbtizae4LcxlNLFsfnhh255fZll06IiIivjBwV37/7noxWAakenfNIVK1cGaNGfzXrKTSgj/qa/Y1L/jnee/fdGD/ugnj77bdj0GcOiV/95uEoKSnJanKjUe+YWLx4cRx55P/9YU+Y8Lc/7FGjRsU999zTYMN2BUM/e0S8+34u6xlk5OE5j2U9gTwZdkyFv9u7oI/6ml1UVBRXXnV1XHnV1YUbtZOod0wcccQRkcv5SwQA/I2fzQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAEASMQEAJBETAECS5lkdeM4jD8eqVVVZHZ4CeHLBE1v9yq7LuW46nOum48U/vfCxH1uUy+VyedyyjZqamigtLS3kIQFoQEUtiiP3fm3WMyiQ6urqaNeu3XYfk9mVicsnTope+/fO6vAUwJMLnojp06bGOeeeH+WDD8t6DnnkXDcdH5zrPU/4RrTYsyzrOeRR7Zrl8fbv7vxYj80sJo4eVhFDhg7N6vAUyPRpU6N88GFx2ulnZD2FPHOum47p06ZGiz3LorhTz6ynkEebN773sR/rDZgAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxAQAkERMAQBIxkUc331gZhx06KD6xR0l07dwhTjl5RLz04otZzyIPrrv26mjdomirj25dOmU9izzYtGlTXH3VlbH/vt1jj5LW0Xu/T8UN110bmzdvznoaid6r+s9Y++/XxKrvnhmv3HhC/PWlJ7f6/F9fXBCvz/yXqLrj9HjlxhNi4+t/yWhp41OvmKisrIxBgwZFSUlJdOjQIUaMGBEvenL8UI/PmxvnnT825s5fGL/67eyo27QpTjiuIjZs2JD1NPLggD59YkXV6i0fi5Y+m/Uk8uA7N98Yd03/Xtx6+52x7NkX4vrKm+LW79wcU+7816ynkSi38b1o0eFT0X7Yef/w85vffy+KuxwQu392VIGXNX7N6/PguXPnxtixY2PQoEGxadOmmDRpUlRUVMTzzz8fbdq0ydfGndasXz+01e1pd82Irp07xNKnlsSQw4dmtIp8ad6seXTq5GrEru4PC5+ME048KYYfd3xEROzTrVv8bOZP46klizNeRqrWPQZG6x4DP/TzbfseFRERm6pfL9SknUa9rkw89NBDMXr06OjTp0/0798/ZsyYEStXrowlS5bka98upaa6OiIi9tijfcZLyIfly1+O7l07x/77do+RZ5wWK/7iEuiuqPywIfH738+Jl196KSIinnn66XjyifnxueHHZbwMslOvKxN/r/p/nxzbt/fk+FFyuVxcdumEGHzYkOjTt2/Wc2hggz5zSNw140ex7777xdq1r8fkG66LI4cOjiVPPxd77rln1vNoQJdcelnUVFdH/777R7NmzaKuri6u+db1ceppX856GmRmh2Mil8vFhAkTYsiQIdF3O0+OtbW1UVtbu+V2TU3Njh5yp3bxRRfGs88+E3Mem5/1FPLgc8cO/3+3DoxDDi2PPr16xE9+9MP4+sUTMttFw/v5z2bGT+/9Sdzz43vjgAP6xDNPL4tLvzE+9t67c3zlTN9Lp2na4Zi48MIL45lnnon587f/5FhZWRnXXHPNjh5ml3Dx18fFr341Kx55dF506dIl6zkUQJs2baJP3wPjz8tfznoKDeyKyy+NSy69PL506mkREdH3wANj5cpX4uabKsUETdYO/dPQcePGxaxZs+L3v//9Rz45Tpw4Maqrq7d8VFVV7dDQnVEul4vxF10YDz7wy3jo4UejW/fuWU+iQGpra+NPf3ohOu29d9ZTaGDv/vWvsdtuW3/pbNasmX8aSpNWrysTuVwuxo0bF/fff3889thj0f1jPDkWFxdHcXHxDg/cmY0fNzZm3ndv/PyXD0bbkpJYs2ZNRESUlpZG69atM15HQ7r8ny+J4084McrKusbatWvjxsrrYl1NTZwx0ivVXc1xx58YN06+Psq6do0DDugTy5YtjTtuuyXOHP3VrKeRaPPGd2PT26u33N5U/XpsfP0vsVvrttG8XYeoe3dd1NW8EXXr34qIiPf/e1VERDRrs0c0a7tHJpsbi3rFxNixY+Pee++NBx98MEo8OX6k6dOmRkRExdFHbH3/XTNi5KjRhR9E3rz66qo48ytfjrfefDP2+sQn4jOHHBpz5y+MffbZJ+tpNLBbbv/XuOab/xJfH3dBvLF2bezduXOM+dq5ccWVV2U9jUQb17wcr//0ii233370roiIaNP36Njr+Ivj3eV/iLd+c9uWz78566aIiCg97Mux+5AzCrq1salXTEyd+rcnxyOOOGKr+2fMmBGjR49uqE27jHffz2U9gQL58b/dl/UECqSkpCS+fctt8e1bbst6Cg2sVdd+sc9lv/rQz7c9cFi0PXBYARftPOr9bQ4AgP/Pz+YAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgiZgAAJKICQAgSfNCHzCXy0VExNPPLCv0oSmwF//0wpZf58+bl/Ea8sm5bjo+ONe1a5bH5o3vZbyGfNq49i8R8X/P29tTlPs4j2pAq1atirKyskIeEgDYQVVVVdGlS5ftPqbgMbF58+Z47bXXoqSkJIqKigp56MzU1NREWVlZVFVVRbt27bKeQx45102L8910NMVzncvlYt26ddG5c+fYbbftvyui4N/m2G233T6ycHZV7dq1azL/ETZ1znXT4nw3HU3tXJeWln6sx3kDJgCQREwAAEnERAEUFxfHN7/5zSguLs56CnnmXDctznfT4VxvX8HfgAkA7FpcmQAAkogJACCJmAAAkogJACCJmAAAkogJACCJmAAAkogJACDJ/wBxaUobmW0V0gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us plot the gridworld\n",
    "g = MDPGridworld(3, 4)\n",
    "g.state(2,3).is_terminal = True\n",
    "ids = g.all_state_ids()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(ids, cmap='Blues', vmin=0, vmax=0)\n",
    "for i in range(ids.shape[0]):\n",
    "    for j in range(ids.shape[1]):\n",
    "        c = ids[i,j]\n",
    "        ax.text(j, i, f'{c:.0f}', va='center', ha='center')\n",
    "\n",
    "        if g.state(i,j).is_terminal:\n",
    "            r = plt.Rectangle((j-0.5,i-0.5), 1, 1, fill=True)\n",
    "            ax.add_patch(r)\n",
    "        \n",
    "        r = plt.Rectangle((j-0.5,i-0.5), 1, 1, fill=False)\n",
    "        ax.add_patch(r)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9a6b0-0d39-4666-80e9-57f6f0b730d6",
   "metadata": {},
   "source": [
    "The sample_episode method that you have defined earlier should already work for the states in the gridworld (if not, go back and check how you sample from the action, here you have 4 actions, while you had only 2 before).\n",
    "\n",
    "\n",
    "Define a uniformely random policy for the gridworld and evaluate how long it will take to reach the terminal state from state(0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28fd87fc-85c8-4909-b49c-55f026f320f1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d54cd4b08d5342307efe9daa6acfbf99",
     "grade": false,
     "grade_id": "cell-040fc8421b31fbd0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Actions are up, down, left, right [N, S, W, E]\n",
    "policy = np.full((12, 4), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0144bd-cc0b-437c-873a-d1b00b52d29a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b72f0f55820178b25fd5dc69d706c69a",
     "grade": true,
     "grade_id": "cell-02262923d966a1c1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.32\n"
     ]
    }
   ],
   "source": [
    "start = g.state(0,0)\n",
    "\n",
    "r = evaluate_policy(policy, start)\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33b67f-0197-4035-b9f8-80c97ac10178",
   "metadata": {},
   "source": [
    "## Bonus exercise: Slippery Gridworld\n",
    "\n",
    "Using the gridworld above, we want to define a class for the slippery gridworld introduced in the lecture.\n",
    "\n",
    "On a slippery grid cell, the specified probability defines the probability to fall to the next south position (x0+1). Add the body of the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9696689b-479a-4ea7-81b6-2815ac5d2273",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e5bbe055a1ca031c27b6bf69adfd02e",
     "grade": false,
     "grade_id": "cell-3cc7f754129de104",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SlipperyGridworld(MDPGridworld):\n",
    "    def add_slippery_patch(self, x0, x1, probability):\n",
    "        \"\"\"\n",
    "        Add a patch of slippery ground to the gridworld. This means that the agent has a chance of slipping down\n",
    "        one grid position on any of the actions (N, E, W) taken\n",
    "        Args:\n",
    "            x0: the x0 coordinate of the patch\n",
    "            x1: the x1 coordinate of the patch\n",
    "            probability: the probability to slip and fall one grid position down (S)\n",
    "        \"\"\"\n",
    "        state = self.state(x0, x1)\n",
    "        \n",
    "        # For actions N, E, W: add a transition to slip south (x0+1)\n",
    "        for action in [self.N, self.E, self.W]:\n",
    "            # Determine where slipping takes us\n",
    "            if x0 < self._height - 1:\n",
    "                # Can slip to the south\n",
    "                slip_state = self.state(x0 + 1, x1)\n",
    "            else:\n",
    "                # At bottom edge, stay in same position when slipping\n",
    "                slip_state = state\n",
    "            \n",
    "            # Add the slip transition with rescaling\n",
    "            state.add_transition_rescale(action, slip_state, -1.0, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ac756f8-2016-4b1a-a84e-b211810adc1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6750acf63ac9a0e2068ee753a819b58d",
     "grade": true,
     "grade_id": "cell-29a2d553a0fd2662",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'probability': 0.8, 'next_state': <__main__.RState object at 0x7f4368efec00>, 'reward': -1.0}, {'probability': 0.2, 'next_state': <__main__.RState object at 0x7f4368efd580>, 'reward': -1.0}]\n"
     ]
    }
   ],
   "source": [
    "gs = SlipperyGridworld(6,3)\n",
    "gs.add_slippery_patch(3, 2, 0.2)\n",
    "transitions = gs._states[gs.pos_to_id(3, 2)]._transitions[gs.W]\n",
    "print(transitions)\n",
    "assert len(transitions) == 2\n",
    "assert transitions[0]['probability'] == 0.8\n",
    "assert transitions[1]['probability'] == 0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
