{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af228dc-9528-469e-9b29-36f26b339430",
   "metadata": {},
   "source": [
    "# Policy Based Method\n",
    "\n",
    "In this exercise we want to explore policy-based methods and develop a simple hill-climbing approach to solve a reinforcement learning problem.\n",
    "\n",
    "In order to develop the algorithm, we need:\n",
    "* a function approximation (neural network) to calculate the _policy_ from the observation,\n",
    "* to sample episodes and calculate the returns (or a similar measure), and\n",
    "* change the network weights using noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0fbd63-342b-4c84-9927-55ee5f34b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /opt/conda/lib/python3.12/site-packages (0.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "import jdc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f38177-2e5f-4ab2-a8fe-3498eb2531ae",
   "metadata": {},
   "source": [
    "## Build the model for the policy \n",
    "\n",
    "Our agent will again use a function to generate the neural network (the model), so that we can call it using different models. For torch we will define this as class that encapsulates the network for the policy (so derived from `nn.Module` and uses a method act to calculate the action which can then be used directly in the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32154481-876b-409b-8710-ac48ea5e31e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a399bf8f1384d6c0724c5a350942a1f6",
     "grade": false,
     "grade_id": "cell-9022b0c42223a457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # define a function self.fc that contains the network using nn.Sequential\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(observation_space.shape[0], 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, action_space.n),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # calculate the action and return it\n",
    "        obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        action_probs = self.forward(obs_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85094c59-098b-465c-a80c-1da67ec46958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d733b36e0138d8bb714f8c74142dcca",
     "grade": true,
     "grade_id": "cell-5bf2acd11b27a4a3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[0.4325, 0.5675]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "obs_sample = env.observation_space.sample()\n",
    "action = policy.act(obs_sample)\n",
    "print(action)\n",
    "action_prob = policy.forward(torch.from_numpy(obs_sample).float().unsqueeze(0))\n",
    "print(action_prob)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0e04f-1a0c-4945-8271-a6a9b3cd841d",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. We will start with the class definition and the `__init__` method Check the parameters and the descriptions as they will be used in the implementation.\n",
    "\n",
    "We have two attributes in the class that save the best weights and the corresponding best return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e40388d-9b40-432e-8b76-808ecdfc2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HillClimbingAgent:\n",
    "    \"\"\"\n",
    "    Implementation of a hill climbing reinforcement learning agent.\n",
    "\n",
    "    The weights of the neural networks are perturbed randomly using noise, if the returns are larger, the new weights\n",
    "    are kept if not the old weights are restored.\n",
    "\n",
    "    The noise of the random changes are diminished if the agent got better and increased if not\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Initialize agent\n",
    "        Args:\n",
    "            observation_space: the observation space of the environment\n",
    "            action_space: the action space of the environment\n",
    "            gamma: the discount factor\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # additional hyperparameters\n",
    "        self.min_noise_scale = 0.001\n",
    "        self.max_noise_scale = 2.0\n",
    "\n",
    "        # noise scaling\n",
    "        self.noise_scale = 1.0\n",
    "        \n",
    "        # generate the model\n",
    "        self.policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "\n",
    "        # array to store the rewards for calculating the return\n",
    "        self.rewards = []\n",
    "\n",
    "        # array to store the weights of the model\n",
    "        self.best_weights = [param.data for param in self.policy.parameters()]\n",
    "        self.best_return = -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b4c91-576c-4689-b1d0-34d9bf306abc",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "Next we implement the calculation of the action. In torch we just have to call the appropriate method from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633df8f1-0628-4c9a-8c2d-fbcb5c0e7d46",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca4a052bbe06c1f1319f082a935c7de6",
     "grade": false,
     "grade_id": "cell-a30d2555c7934205",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "\n",
    "def calculate_action(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action to take\n",
    "    Args:\n",
    "        obs: the observation\n",
    "    Returns:\n",
    "        the action to take\n",
    "    \"\"\"\n",
    "    return self.policy.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131bad45-29b4-4b5e-8916-36b5b41b2297",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b49fe8fac69e351f732e192b6a3a63e",
     "grade": true,
     "grade_id": "cell-b3d7385f75b4564d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "obs_sample = env.observation_space.sample()\n",
    "agent = HillClimbingAgent(observation_space=env.observation_space, \n",
    "                          action_space=env.action_space)\n",
    "\n",
    "action = agent.calculate_action(obs_sample)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851fd08-2204-4df5-8dbd-c3f0fdeca5df",
   "metadata": {},
   "source": [
    "### Step functions and training\n",
    "\n",
    "Next we will add the step functions and the _training_ inside them. \n",
    "\n",
    "Simular to MC methods, updates only occur at the end of episodes. For the update, you have to check if the return is better than the best return so far. If yes the new weights are stored as best weights, if not, the previous best weights are restored.\n",
    "\n",
    "The current set of weights is then changed by adding random, normally distributed noise which is multiplied by the current noise scale value.\n",
    "\n",
    "This value should be divided by a factor of 2 if the return was better, or multiplied by a factor of 2 if the return was not. The noise should not exceed `self.max_noise_scale` or become smaller than `self.min_noise_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ddd40f-a0c6-4dec-a4da-653c2b3e2111",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9153e436bd0ad0e2f0d0683e8a7d1ed",
     "grade": false,
     "grade_id": "cell-254041610abfba8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    action = self.calculate_action(obs)\n",
    "    return action\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    # simular to MC learning, we only update at the end of an episode\n",
    "\n",
    "    # append the reward from the last time step\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "    if not done:\n",
    "        # we have to do the same as in the first_step: calculate the action\n",
    "        return self.step_first(obs)\n",
    "\n",
    "    else:\n",
    "        # an episode is finished, so we can calculate the return and update the weights\n",
    "        current_return = sum([self.gamma ** i * reward for i, reward in enumerate(self.rewards)])\n",
    "        \n",
    "        # Check if current return is better than best return\n",
    "        if current_return > self.best_return:\n",
    "            # Update best return and save current weights as best weights\n",
    "            self.best_return = current_return\n",
    "            self.best_weights = [param.data.clone() for param in self.policy.parameters()]\n",
    "            # Reduce noise scale (we're doing well, make smaller changes)\n",
    "            self.noise_scale = max(self.noise_scale / 2.0, self.min_noise_scale)\n",
    "        else:\n",
    "            # Restore previous best weights (current weights didn't improve)\n",
    "            for param, best_weight in zip(self.policy.parameters(), self.best_weights):\n",
    "                param.data = best_weight.clone()\n",
    "            # Increase noise scale (we need to explore more)\n",
    "            self.noise_scale = min(self.noise_scale * 2.0, self.max_noise_scale)\n",
    "        \n",
    "        # Add random noise to the weights for next episode\n",
    "        for param in self.policy.parameters():\n",
    "            noise = torch.randn_like(param.data) * self.noise_scale\n",
    "            param.data.add_(noise)\n",
    "\n",
    "        # reset the rewards\n",
    "        del self.rewards[:]\n",
    "\n",
    "        # return None, as there is no action from a terminal state\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36de5afb-f3f7-459a-be76-79d8dbd329c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d513f1fc74dce97df6c203ae2b2c6cff",
     "grade": true,
     "grade_id": "cell-e3cf687afa6e3d4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "hill_climbing_agent = HillClimbingAgent(env.observation_space, \n",
    "                                        env.action_space,\n",
    "                                        gamma=0.99)\n",
    "\n",
    "# Check if one complete episode runs through\n",
    "obs, _ = env.reset()\n",
    "action = hill_climbing_agent.step_first(obs)\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    action = hill_climbing_agent.step(obs, reward, done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9eb625-4c9f-4ac5-9f68-ea1e5d141baa",
   "metadata": {},
   "source": [
    "### Training and evaluation\n",
    "\n",
    "We add the train and evaluate methods in the agents, similar to the last exercise so that it is easier to run some tests. Nothing to code here. Note that the number of steps for training are episodes here, as we only change the weights at the end of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a94f74-276d-4fe9-b32c-c1d695b5da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to HillClimbingAgent\n",
    "def train(self, env: gym.Env, \n",
    "          nr_episodes_train: int, \n",
    "          eval_env: gym.Env, \n",
    "          eval_frequency: int, \n",
    "          eval_nr_episodes: int,\n",
    "          eval_gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_train: the number of episodes to train\n",
    "        eval_env: the environment to use for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent (in episodes)\n",
    "        eval_nr_episodes: The number of episodes to evaluate\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, eval_nr_episodes, eval_gamma)\n",
    "            print(f'Evaluation: Episode trained {nr_episodes}, mean reward: {np.mean(rewards)}')\n",
    "        \n",
    "        if nr_episodes > nr_episodes_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a = self.calculate_action(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        gamma_current = gamma\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.calculate_action(obs)\n",
    "            episode_reward += gamma_current * reward\n",
    "            gamma_current *= gamma\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0fd60-4646-4fcb-9867-c386c8148a0b",
   "metadata": {},
   "source": [
    "We train the agent for a number of steps to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f759a721-3a76-422a-8596-c3e04f1d2e2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec624904f8eb74e75c160f0562d9044",
     "grade": true,
     "grade_id": "cell-cf2ec3e64d0c16f4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Episode trained 50, mean reward: 11.0\n",
      "Evaluation: Episode trained 100, mean reward: 500.0\n",
      "Evaluation: Episode trained 150, mean reward: 500.0\n",
      "Evaluation: Episode trained 200, mean reward: 500.0\n",
      "Evaluation: Episode trained 250, mean reward: 500.0\n",
      "Evaluation: Episode trained 300, mean reward: 500.0\n",
      "Evaluation: Episode trained 350, mean reward: 500.0\n",
      "Evaluation: Episode trained 400, mean reward: 500.0\n",
      "Evaluation: Episode trained 450, mean reward: 500.0\n",
      "Evaluation: Episode trained 500, mean reward: 500.0\n",
      "Evaluation: [500.0]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(1)\n",
    "\n",
    "# Hyperparameters will be quite important here.\n",
    "hill_climbing_agent = HillClimbingAgent(env.observation_space, \n",
    "                                        env.action_space,\n",
    "                                        gamma=1.0)\n",
    "\n",
    "hill_climbing_agent.train(env, nr_episodes_train=500, \n",
    "                          eval_env=eval_env, eval_frequency=50, eval_nr_episodes=5, eval_gamma=1.0)\n",
    "\n",
    "# calculate return at end using evaluation\n",
    "return_eval = hill_climbing_agent.evaluate(env=eval_env, nr_episodes=1, gamma=1.0)\n",
    "\n",
    "print(f'Evaluation: {return_eval}')\n",
    "\n",
    "# The algorithms should usually the correct solution (500) within those training episodes, but it is not guaranteed :-)\n",
    "\n",
    "assert return_eval[0] > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40856c96-4234-4e99-bec3-df6b23e4cf8f",
   "metadata": {},
   "source": [
    "Congratulation, you have implemented your first policy-based algorithm, lets have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2359d86-8651-4965-b461-c735d2271510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with a few steps\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "for _ in range(499):\n",
    "    action = hill_climbing_agent.calculate_action(obs)\n",
    "    obs, _, done, _, _ = env.step(action) \n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31427089-7287-409e-8058-06b78fc8e176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
