{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af228dc-9528-469e-9b29-36f26b339430",
   "metadata": {},
   "source": [
    "# REINFORCE: Vanilla Policy Gradient\n",
    "\n",
    "In this exercise we want to implement the REINFORCE policy-gradient method to solve a reinforcement learning problem.\n",
    "\n",
    "In order to develop the algorithm, we need:\n",
    "* a function approximation (neural network) to calculate the _policy_ from the observation,\n",
    "* to sample episodes and calculate the returns (or a similar measure),\n",
    "* calculate the gradient of the return (involves calculating the gradients of the policy)\n",
    "* apply gradient ascent to change the weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0fbd63-342b-4c84-9927-55ee5f34b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc in /opt/conda/lib/python3.12/site-packages (0.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f38177-2e5f-4ab2-a8fe-3498eb2531ae",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Our agent will again use a function to generate the neural network (the model), so that we can call it using different models. For torch we will define this as class that encapsulates the network for the policy (so derived from `nn.Module` and uses a method act to calculate the action which can then be used directly in the agent.\n",
    "\n",
    "For the loss calculation later, we will need not only the selected action, but also the (log) probability of the selected action and the gradient on it. We should make it possible to save this from the policy network. Implement the `act` method so that it return both the selected action and the log of the probability of choosing this action. You can use the method `log_prob` from `Categorical`. For the log_prob, return the tensor from torch instead of the value (i.e. do not use `item`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32154481-876b-409b-8710-ac48ea5e31e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1942b77b26f2cf4528eaa6b1c1bc5be",
     "grade": false,
     "grade_id": "cell-9022b0c42223a457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # define a function self.fc that contains the network using nn.Sequential\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, obs):\n",
    "        # calculate the action and return it\n",
    "        obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "        action_probs = self.forward(obs_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a15105d-3f30-4196-a294-b9a18bfafcf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5182edeed8596469e4aa2b2297d911a8",
     "grade": true,
     "grade_id": "cell-b5a410bcf61bdea9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([-0.3352], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[0.7152, 0.2848]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "policy = PolicyNetwork(env.observation_space, env.action_space)\n",
    "obs_sample = env.observation_space.sample()\n",
    "action, log_prob = policy.act(obs_sample)\n",
    "print(action, log_prob)\n",
    "action_prob = policy.forward(torch.from_numpy(obs_sample).float().unsqueeze(0))\n",
    "print(action_prob)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0e04f-1a0c-4945-8271-a6a9b3cd841d",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. We will start with the class definition and the `__init__` method. Check the parameters and the descriptions as they will be used in the implementation. There is one additional array `log_prob` to save the log probabilities of the actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e40388d-9b40-432e-8b76-808ecdfc2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGAgent:\n",
    "    \"\"\"\n",
    "    Implementation of (vanilla) policy gradient agent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 gamma: float = 0.99,\n",
    "                 learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize agent\n",
    "        Args:\n",
    "            observation_space: the observation space of the environment\n",
    "            action_space: the action space of the environment\n",
    "            gamma: the discount factor\n",
    "            learning_rate: the learning rate\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # generate the model\n",
    "        self.policy_network = PolicyNetwork(observation_space, action_space)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # arrays to store an episode for training\n",
    "        self.obs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b4c91-576c-4689-b1d0-34d9bf306abc",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "The model directly calculates the policy, so we just have to draw an action from the resuling probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633df8f1-0628-4c9a-8c2d-fbcb5c0e7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def calculate_action(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action to take\n",
    "    Args:\n",
    "        obs: the observation\n",
    "    Returns:\n",
    "        the action to take, the log probability of the action\n",
    "    \"\"\"\n",
    "    return self.policy_network.act(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131bad45-29b4-4b5e-8916-36b5b41b2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_sample = env.observation_space.sample()\n",
    "agent = VPGAgent(observation_space=env.observation_space, \n",
    "                          action_space=env.action_space)\n",
    "\n",
    "action, log_prob = agent.calculate_action(obs_sample)\n",
    "assert action == 0 or action == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851fd08-2204-4df5-8dbd-c3f0fdeca5df",
   "metadata": {},
   "source": [
    "## Step functions and training\n",
    "\n",
    "Next we will add the step functions and the _training_ inside them. \n",
    "\n",
    "### step first\n",
    "\n",
    "The step first function just calculates and action appends the information for our episode (observation, action). The policy is stochastic, so we should draw from the random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bd95f4-4cc3-47d2-a2fa-a39d0a3952aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    self.obs.append(obs)\n",
    "    action, log_prob = self.calculate_action(obs)\n",
    "    self.actions.append(action)\n",
    "    self.log_probs.append(log_prob)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b7763-dafc-4514-949a-8fd155378474",
   "metadata": {},
   "source": [
    "### Step and training\n",
    "\n",
    "\n",
    "Simular to MC methods, updates only occur at the end of episodes and we only use the calculated return once for the gradient, however we do this for each of the actions during this episode. Therefor the update batch has the length of an episode.\n",
    "\n",
    "The update of the gradients is according to\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta J(\\pi_\\theta)\n",
    "$$\n",
    "\n",
    "where $J(\\pi_\\theta)$ is the loss function. In general the gradient of the loss functionhas the form\n",
    "\n",
    "$$\n",
    "\\nabla\\theta J(\\pi\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^T \\nabla_\\theta\\Phi_t\\log\\pi_\\theta(a_t| s_t) \\right]\n",
    "$$\n",
    "\n",
    "where there are different choices for $\\Phi_t$. We can for example use\n",
    "\n",
    "$$\n",
    "\\Phi_t = G\n",
    "$$\n",
    "where $G$ is the (total) return of the episode, or use the obtained return from each state, sometimes also called the sum of the discounted future rewards.\n",
    "$$\n",
    "\\Phi_t = \\sum_{t'=t}^T R_t\n",
    "$$\n",
    "It can be proven, that all these choices actually lead to the same expectation of the gradient. I would suggest to use the sum of discounted future rewards.\n",
    "\n",
    "It can also help to normalize the values to zero mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ddd40f-a0c6-4dec-a4da-653c2b3e2111",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f6e7533fdf7da3ec7a52ee3be12f327",
     "grade": false,
     "grade_id": "cell-254041610abfba8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    # simular to MC learning, we only update at the end of an episode\n",
    "\n",
    "    # udpate the reward from the last time step (so that all arrays should now have the same length)\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "    if not done:\n",
    "        # we have to do the same as in the first_step: add the observation and calculate and store an action\n",
    "        return self.step_first(obs)\n",
    "\n",
    "    else:\n",
    "        # an episode is finished, so we calculate the gradient and update the weights\n",
    "        assert len(self.obs) == len(self.actions)\n",
    "        assert len(self.obs) == len(self.rewards)\n",
    "        assert len(self.obs) == len(self.log_probs)\n",
    "\n",
    "        future_rewards = np.zeros_like(self.rewards)\n",
    "\n",
    "        # Calculate discounted future rewards for each timestep\n",
    "        running_sum = 0\n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            running_sum = self.rewards[t] + self.gamma * running_sum\n",
    "            future_rewards[t] = running_sum\n",
    "        \n",
    "        # Normalize the returns (optional but helps with stability)\n",
    "        future_rewards = (future_rewards - np.mean(future_rewards)) / (np.std(future_rewards) + 1e-9)\n",
    "        \n",
    "        # Calculate the policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, reward in zip(self.log_probs, future_rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        \n",
    "        # Stack all losses and compute mean\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Perform backpropagation and optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        del self.rewards[:]\n",
    "        del self.obs[:]\n",
    "        del self.actions[:]\n",
    "        del self.log_probs[:]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36de5afb-f3f7-459a-be76-79d8dbd329c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b1e621871255da246a96be347714680",
     "grade": true,
     "grade_id": "cell-e3cf687afa6e3d4c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space, \n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "# Check if one complete episode runs through\n",
    "obs, _ = env.reset()\n",
    "action = agent.step_first(obs)\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9eb625-4c9f-4ac5-9f68-ea1e5d141baa",
   "metadata": {},
   "source": [
    "### Training and evaluation\n",
    "\n",
    "We add the train and evaluate methods in the agents, similar to the last exercise so that it is easier to run some tests. Nothing to code here. Note that the number of steps for training are episodes here, as we only change the weights at the end of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a94f74-276d-4fe9-b32c-c1d695b5da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPGAgent\n",
    "def train(self, env: gym.Env, \n",
    "          nr_episodes_train: int, \n",
    "          eval_env: gym.Env, \n",
    "          eval_frequency: int, \n",
    "          eval_nr_episodes: int,\n",
    "          eval_gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_train: the number of episodes to train\n",
    "        eval_env: the environment to use for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent (in episodes)\n",
    "        eval_nr_episodes: The number of episodes to evaluate\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset()\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a = self.step(obs, reward, done or truncated)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, eval_nr_episodes, eval_gamma)\n",
    "            print(f'Evaluation: Episode trained {nr_episodes}, mean reward: {np.mean(rewards)}')\n",
    "        \n",
    "        if nr_episodes > nr_episodes_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, gamma: float = 1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        a,_ = self.calculate_action(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        gamma_current = gamma\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            a, _ = self.calculate_action(obs)\n",
    "            episode_reward += gamma_current * reward\n",
    "            gamma_current *= gamma\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0fd60-4646-4fcb-9867-c386c8148a0b",
   "metadata": {},
   "source": [
    "We train the agent for a number of steps to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f759a721-3a76-422a-8596-c3e04f1d2e2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4c843c839e12e50e11a4cf3a76e51b4",
     "grade": true,
     "grade_id": "cell-cf2ec3e64d0c16f4",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Episode trained 25, mean reward: 41.0\n",
      "Evaluation: Episode trained 50, mean reward: 27.0\n",
      "Evaluation: Episode trained 75, mean reward: 37.0\n",
      "Evaluation: Episode trained 100, mean reward: 48.0\n",
      "Evaluation: Episode trained 125, mean reward: 23.0\n",
      "Evaluation: Episode trained 150, mean reward: 73.0\n",
      "Evaluation: Episode trained 175, mean reward: 40.0\n",
      "Evaluation: Episode trained 200, mean reward: 90.0\n",
      "Evaluation: Episode trained 225, mean reward: 186.0\n",
      "Evaluation: Episode trained 250, mean reward: 150.0\n",
      "Evaluation: Episode trained 275, mean reward: 96.0\n",
      "Evaluation: Episode trained 300, mean reward: 203.0\n",
      "Evaluation: Episode trained 325, mean reward: 158.0\n",
      "Evaluation: Episode trained 350, mean reward: 243.0\n",
      "Evaluation: Episode trained 375, mean reward: 170.0\n",
      "Evaluation: Episode trained 400, mean reward: 57.0\n",
      "Evaluation: Episode trained 425, mean reward: 241.0\n",
      "Evaluation: Episode trained 450, mean reward: 489.0\n",
      "Evaluation: Episode trained 475, mean reward: 187.0\n",
      "Evaluation: Episode trained 500, mean reward: 500.0\n",
      "Evaluation: Episode trained 525, mean reward: 304.0\n",
      "Evaluation: Episode trained 550, mean reward: 211.0\n",
      "Evaluation: Episode trained 575, mean reward: 301.0\n",
      "Evaluation: Episode trained 600, mean reward: 300.0\n",
      "Evaluation: Episode trained 625, mean reward: 500.0\n",
      "Evaluation: Episode trained 650, mean reward: 500.0\n",
      "Evaluation: Episode trained 675, mean reward: 101.0\n",
      "Evaluation: Episode trained 700, mean reward: 484.0\n",
      "Evaluation: Episode trained 725, mean reward: 310.0\n",
      "Evaluation: Episode trained 750, mean reward: 500.0\n",
      "Evaluation: Episode trained 775, mean reward: 222.0\n",
      "Evaluation: Episode trained 800, mean reward: 162.0\n",
      "Evaluation: Episode trained 825, mean reward: 500.0\n",
      "Evaluation: Episode trained 850, mean reward: 330.0\n",
      "Evaluation: Episode trained 875, mean reward: 500.0\n",
      "Evaluation: Episode trained 900, mean reward: 500.0\n",
      "Evaluation: Episode trained 925, mean reward: 424.0\n",
      "Evaluation: Episode trained 950, mean reward: 500.0\n",
      "Evaluation: Episode trained 975, mean reward: 369.0\n",
      "Evaluation: Episode trained 1000, mean reward: 432.0\n",
      "Evaluation: [339.0]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "eval_env = gym.make(environment_name)\n",
    "\n",
    "obs, info = env.reset()\n",
    "np.random.seed(0)\n",
    "\n",
    "agent = VPGAgent(env.observation_space, \n",
    "                 env.action_space,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.001)\n",
    "\n",
    "agent.train(env, nr_episodes_train=1000, eval_env=eval_env, eval_frequency=25, eval_nr_episodes=1, eval_gamma=1.0)\n",
    "\n",
    "# calculate return at end using evaluation\n",
    "return_eval = agent.evaluate(env=eval_env, nr_episodes=1, gamma=1.0)\n",
    "\n",
    "print(f'Evaluation: {return_eval}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5b3047-4194-41d2-9fbe-36fb01ed0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for longer (1000 episodes) should obtain quite good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d816060f-2106-4126-b51f-9ded36194e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _  = env.reset()\n",
    "for _ in range(200):\n",
    "    action, _ = agent.calculate_action(obs)\n",
    "    obs, _, _, _,_ = env.step(action)  # Take a random action\n",
    "    display_environment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f3f6a-a9e9-4291-8d6f-b6e29e67300a",
   "metadata": {},
   "source": [
    "Congratulations, you implemented a full policy gradient algorithm!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
