{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccbf74e-1fcb-40c2-98d7-c30d0f73f252",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "In this exercise we will build a deep Q-learning agent from scratch. For this we need to look at the following tasks:\n",
    "* How do we specify the model?\n",
    "* How do we calculate an action?\n",
    "* How do we sample episodes?\n",
    "* How do we train the model?\n",
    "\n",
    "We will develop this by implementing a class for the agent. We will use a package called jdc, that will allow us to split the implementation of a class over several cells.\n",
    "\n",
    "We will be using torch for the implementation of the neural network and the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18836b84-bfcc-4d2e-a25c-cd1d9dbef9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jdc\n",
      "  Downloading jdc-0.0.9-py2.py3-none-any.whl.metadata (817 bytes)\n",
      "Downloading jdc-0.0.9-py2.py3-none-any.whl (2.1 kB)\n",
      "Installing collected packages: jdc\n",
      "Successfully installed jdc-0.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install jdc\n",
    "import jdc\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d78c58-adcf-4645-9916-1a3316121f12",
   "metadata": {},
   "source": [
    "## Example: Cart Pole\n",
    "We will use an environment from OpenAI for this exercise. The goal is to balance a pole by moving the attached cart to the left or to the right. As the pole should be balanced as long as possible, the reward for each time step is +1. An episode is done when the angle of the pole becomes too large.\n",
    "\n",
    "The observation space gives some measurements about the pole, for example the angle. However, we actually do not need to know the specific details, as the neural network will just learn using the input.\n",
    "\n",
    "The actions are to move the cart to the left or to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6fccfa-3da9-4939-a71e-f1ae82ceb64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n",
      "Sample from the observation space: [-1.5747947  -0.11998001 -0.3479481   1.2527854 ]\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env.observation_space}')\n",
    "print(f'Action space: {env.action_space}')\n",
    "\n",
    "print(f'Sample from the observation space: {env.observation_space.sample()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6bac-94a9-42ec-ad59-9a5bade325e3",
   "metadata": {},
   "source": [
    "The environment has a render function that we can use to display the state. The parameter 'render_mode' is used to specify the mode. For standalone applications, we can use 'human' that will open a window. Here we get the image as an array and display it using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20961d56-4fe5-45d4-b67b-b791cb864706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR is invalid or not set in the environment.\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1342:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5204:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5727:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2721:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc811ef-a89b-4dd0-b385-c199d8a535d5",
   "metadata": {},
   "source": [
    "We can also try to render a sequence of images. Depending on your browser the display might not be optimal...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780d4e76-70ea-45b4-9b90-205cf6dbeb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "# Test the function with a few steps\n",
    "env.reset()\n",
    "for _ in range(50):\n",
    "    env.step(env.action_space.sample())  # Take a random action\n",
    "    display_environment(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72df12-f213-4217-84fe-4b18b9bbdb94",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Our agent will use the class of the neural network (the model) as parameter, so that we can call it using different models. The first task is to write a example class that for the neural network. We will pass the number of observation values and the number of actions values as parameters.\n",
    "\n",
    "The model must calculate the q function for each state. We will also need to access those values for evaluation where we do not need the gradient calculation.\n",
    "\n",
    "Build a sequential model using at least two dense layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc9c401-f28d-4ed5-bfde-5990b8f3de02",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4f4206b89ef19b7d4ab280852a1f0bc",
     "grade": false,
     "grade_id": "cell-5f57f2a0435bd064",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, n_obs, n_action):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        # generate a sequential model in a internal variable (for example self.fc)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_obs, 128),      # First hidden layer with 128 neurons\n",
    "            nn.ReLU(),                   # Activation function\n",
    "            nn.Linear(128, 128),         # Second hidden layer with 128 neurons\n",
    "            nn.ReLU(),                   # Activation function\n",
    "            nn.Linear(128, n_action)     # Output layer (Q-values for each action)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_tensor):\n",
    "        # forward should just call your model\n",
    "        return self.fc(x_tensor)\n",
    "\n",
    "    def q_values(self, obs_tensor):\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(obs_tensor)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9787f0e3-966e-4d85-94f8-e3950d45f90a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2c6be9b1a7b853ea3305c9138421434",
     "grade": true,
     "grade_id": "cell-02f08792391930df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNetwork(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = DQNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "print(m)\n",
    "\n",
    "obs_sample = env.observation_space.sample()\n",
    "\n",
    "# models expect a batch of data, so we have to add a dimension and convert it to a tensor\n",
    "obs_batch = np.expand_dims(obs_sample, axis=0)\n",
    "obs_batch_tensor = torch.from_numpy(obs_batch).float()\n",
    "action_values = m.q_values(obs_batch_tensor)\n",
    "assert action_values.shape == (1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acfd08-b0fe-41e2-9ad3-47ef889cfa55",
   "metadata": {},
   "source": [
    "### Device managment in torch\n",
    "\n",
    "In order to use the gpu, we have to move our models to the device. We will also set this as\n",
    "default device, so that any tensors will be initialized on it and we don't have to specify them every time.\n",
    "\n",
    "In the code, we will use _tensor whenever a variable is a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ab168b-042e-403a-8e32-e5d2e98e721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        # test if it worked\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using CUDA device')\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        x = torch.ones(1, device=device)\n",
    "        print('Using MPS device')\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3cfd11-13eb-4fd3-a850-e3a0262446aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cd294-1491-4191-a25b-de8aafcd7762",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "Now we are ready to implement the agent class. Check the parameters and their descriptions as they will be used in the implementation and you will have to find suitable hyperparameters for them.\n",
    "\n",
    "\n",
    "The only thing to fill out here are the optimizer and the loss function. \n",
    "\n",
    "There are different optimizers available, either standand SGD or Adam would be possible and should be initialized with the learning rate given in the parameters.\n",
    "\n",
    "What is the loss function that we have to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fc0213-5dc9-4f23-ab2d-1d44d983a144",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc682665d090e45871c7275589f8d3fd",
     "grade": false,
     "grade_id": "cell-8c19c66d131c3fbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space,\n",
    "                 model_cls,\n",
    "                 device,\n",
    "                 gamma: float,\n",
    "                 epsilon: float, epsilon_decay: float, epsilon_min: float,\n",
    "                 learning_rate: float, training_frequency: int, target_update_frequency: int,\n",
    "                 tau : float, use_double_dqn: bool,\n",
    "                 batch_size: int, memory_size: int):\n",
    "        \"\"\"\n",
    "        Initialise the agent.\n",
    "        Args:\n",
    "            observation_space: The observation space of the environment\n",
    "            action_space: The action space of the environment\n",
    "            model_cls: the class that implements the model,\n",
    "            device: the device to run torch on\n",
    "            gamma: The discount factor\n",
    "            epsilon: The initial epsilon value for the epsilon-greedy policy\n",
    "            epsilon_decay: The decay factor for the epsilon value\n",
    "            epsilon_min: The minimal epsilon value after which it will not be decayed further\n",
    "            learning_rate: The learning rate for the optimizer\n",
    "            training_frequency: The frequency (in steps) of training the model\n",
    "            target_update_frequency: The frequency (in steps) of updating the target model (if not using tau)\n",
    "            tau: weight of the new model in the target update \n",
    "            use_double_dqn: use double q learning\n",
    "            batch_size: The batch size for training (sampled from the memory)\n",
    "            memory_size: The size of the memory for storing experiences\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "    \n",
    "        # hyperparameters from parameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.training_frequency = training_frequency\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "    \n",
    "        self.use_double_dqn = use_double_dqn\n",
    "        self.tau = tau\n",
    "\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "        # internal variables\n",
    "        self.nr_steps = 0\n",
    "    \n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "\n",
    "        self.device = device\n",
    "    \n",
    "        # build the models\n",
    "        # self.model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        # self.target_model = DQNetwork(observation_space.shape[0], action_space.n)\n",
    "        self.model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "        self.target_model = model_cls(self.observation_space.shape[0], self.action_space.n)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model = self.target_model.to(self.device)\n",
    "\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent to the initial state\n",
    "        \"\"\"\n",
    "        self.nr_steps = 0\n",
    "        self.last_action = None\n",
    "        self.last_obs = None\n",
    "        self.nr_training_steps = 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4cf988c-24c5-4b8e-ad91-e79fec78d7b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ee19ca899da78a8af841c665edc8caf",
     "grade": true,
     "grade_id": "cell-6638146a1a48950b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe8a89-c401-40bc-a44e-4052b3a3d396",
   "metadata": {},
   "source": [
    "### Updating of the target model\n",
    "\n",
    "It is important in Q-Learning that the model that is trained and the target model used to calculate the target functions are not the same. The target model should stay fixed for a while or change only slowly.\n",
    "\n",
    "There are two different methods to update the model:\n",
    "- Replace the target model every number of steps\n",
    "- Interpolate between the policy and the target model\n",
    "\n",
    "Implement the update to use the replacement when the parameter tau is equal to 0 and the interpolation (using tau) when tau is greater than 0. You should later experiment with both update functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bcc187-fd5a-4689-ba07-f7e62d128f13",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e02fcb0302750b798ad2be8e7a21010",
     "grade": false,
     "grade_id": "cell-a1cb418924b7cb27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Update the target model with the weights from the current model. There are two possibilities for\n",
    "        updating:\n",
    "        - Replace the target model every number of steps\n",
    "        - Interpolate between the policy and the target model\n",
    "\n",
    "        In both cases the update_target_model method will be called every self.target_update_frequency \n",
    "        number of steps (not training steps). If the update is done by replacement, the value should be\n",
    "        higher, if the update is done by interpolation it can be lower.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        policy_net_state_dict = self.model.state_dict()\n",
    "\n",
    "        if self.tau > 0.0:\n",
    "            # Soft update (interpolation)\n",
    "            target_net_state_dict = self.target_model.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = (self.tau * policy_net_state_dict[key] + \n",
    "                                             (1 - self.tau) * target_net_state_dict[key])\n",
    "            self.target_model.load_state_dict(target_net_state_dict)\n",
    "        else:\n",
    "            # Hard update (complete replacement)\n",
    "            self.target_model.load_state_dict(policy_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbda48da-2108-4a2e-83a8-957b81d59023",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbcce6aa140ca971b00622c3915af067",
     "grade": true,
     "grade_id": "cell-cb441f633638dff8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "q_agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       model_cls=DQNetwork,\n",
    "                       device=device,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=2,\n",
    "                       tau=0.0,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=256,\n",
    "                       memory_size=10000)\n",
    "q_agent.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b09c6-c2b2-45dc-81e2-c87179dab829",
   "metadata": {},
   "source": [
    "## Calculating actions\n",
    "\n",
    "Calculating actions is done with an epsilon greedy policy. However, for evaluation it is often suitable to use the greedy policy instead. So we add a parameter `stochastic`, if it is True then the epsilon-greedy policy is used, if not, the greedy policy is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "808c4b3c-82ec-456a-beb4-f94a3bc1e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def calculate_action(self, obs_tensor, stochastic: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the action for the given observation.\n",
    "    Args:\n",
    "        obs: the observation\n",
    "        stochastic: whether to use a stochastic (epsilon greedy) policy or not\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    if not stochastic or np.random.rand() > self.epsilon:\n",
    "        # calculate greedy action\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model.q_values(obs_tensor)\n",
    "        return torch.argmax(action_value).numpy(force=True)\n",
    "    else:\n",
    "        # calculate random action\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d490aa0b-73f6-4001-8ee5-0387ff91a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "obs_tensor = torch.tensor(env.observation_space.sample())\n",
    "a = agent.calculate_action(obs_tensor)\n",
    "assert env.action_space.contains(a)\n",
    "a = agent.calculate_action(obs_tensor, stochastic=False)\n",
    "assert env.action_space.contains(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cbec0-6b65-40b4-9bbf-c0c136afd366",
   "metadata": {},
   "source": [
    "## Add the steps\n",
    "\n",
    "Next we will add the two step functions as in the previous implementations of an agent. The `step_first` method is called after the environment is reset and we do not have any rewards yet.\n",
    "\n",
    "The `step` method is called for all other steps. In the step method we need to\n",
    "* Save the current experience (S, A, R, S', done) in the memory.\n",
    "* Calculate the next action\n",
    "* Save action and observation for next step\n",
    "* train the model every couple of steps\n",
    "* update the target model every couple of steps\n",
    "\n",
    "You have to fill in the code for the first three items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "922fa8c8-bc5d-4552-8709-91c6a8c9c421",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edc27e1ff54fbb99d45bdecba8e633e3",
     "grade": false,
     "grade_id": "cell-f11d5c8957197292",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def train_model(self):\n",
    "    # the method must exist in the class, or an error occurs\n",
    "    \n",
    "    # we will reimplement this later, issue a warning if it is not\n",
    "    self.nr_training_steps += 1\n",
    "    print('train model not replaced')\n",
    "    \n",
    "def step_first(self, obs):\n",
    "    \"\"\"\n",
    "    Calculate the action for the first step in the environment after a reset.\n",
    "    Args:\n",
    "        obs: The observation from the environment\n",
    "    Returns:\n",
    "        the action\n",
    "    \"\"\"\n",
    "    self.last_obs = obs\n",
    "    obs_tensor = torch.tensor(obs)\n",
    "    self.last_action = self.calculate_action(obs_tensor)\n",
    "    return self.last_action\n",
    "\n",
    "def step(self, obs, reward: float, done: bool):\n",
    "\n",
    "    # now we have a new state and reward from the last action and can add that to the buffer (memory)\n",
    "    # you should then calculate the next action, for this we need a tensor which is created here as example\n",
    "    obs_tensor = torch.tensor(obs)\n",
    "    \n",
    "    # 1. Save the current experience (S, A, R, S', done) in the memory\n",
    "    self.memory.append((self.last_obs, self.last_action, reward, obs, done))\n",
    "    \n",
    "    # 2. Calculate the next action\n",
    "    action = self.calculate_action(obs_tensor)\n",
    "    \n",
    "    # 3. Save action and observation for next step\n",
    "    self.last_action = action\n",
    "    self.last_obs = obs\n",
    "\n",
    "    self.nr_steps += 1\n",
    "\n",
    "    if self.nr_steps % self.training_frequency == 0:\n",
    "        self.train_model()\n",
    "\n",
    "    if self.nr_steps % self.target_update_frequency == 0:\n",
    "        self.update_target_model()\n",
    "\n",
    "    return self.last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "679ad371-d595-4b86-9b6d-322f0d2c3434",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68f17895cc3cd32aa4caf076cf644041",
     "grade": true,
     "grade_id": "cell-55874102f12eeb67",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.9,\n",
    "                 epsilon_min=0.03,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 learning_rate=0.0005,\n",
    "                 training_frequency=1,\n",
    "                 target_update_frequency=2,\n",
    "                 tau=0.005,\n",
    "                 use_double_dqn=False,\n",
    "                 batch_size=256,\n",
    "                 memory_size=10000)\n",
    "a = agent.step_first(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e1f9b-2b90-4d3f-94b7-adf575b31e33",
   "metadata": {},
   "source": [
    "## Train the model function\n",
    "\n",
    "The last thing to do is now to train the model. For this we have to \n",
    "* Sample from the memory to get a batch of observations, actions, rewards, next observations and dones\n",
    "* Calculate a better estimate for the q values of the current observation using q-learning\n",
    "* Fit the model to the updated values using gradient descend on the loss\n",
    "* Decay the epsilon value\n",
    "\n",
    "In torch, the gradient descend step has to be calculated in the code. Todo this:\n",
    "* Calculate the model output\n",
    "* Calculate the loss function\n",
    "* Clear the gradient (using self.optimizer.zero_grad())\n",
    "* Calculate a backwards step (resulting in the gradient)\n",
    "* Apply the step in the optimizer\n",
    "\n",
    "Note that between calculating the model, and calculating the loss function, you will have to calculate the target function (using the target_model), as no gradients are required on the target model, this code should be within a `with torch.no_grad()` block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5506a-0c18-4433-8b1d-529177bc2cf4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642443bea50b66830d541a7458e57b1d",
     "grade": false,
     "grade_id": "cell-fe51b24157ad1aa1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train_model(self):\n",
    "    # not enough samples yet\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "\n",
    "    # 1. Sample a batch from memory\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    # Unpack the batch into separate arrays\n",
    "    states = np.array([experience[0] for experience in batch])\n",
    "    actions = np.array([experience[1] for experience in batch])\n",
    "    rewards = np.array([experience[2] for experience in batch])\n",
    "    next_states = np.array([experience[3] for experience in batch])\n",
    "    dones = np.array([experience[4] for experience in batch])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states_tensor = torch.FloatTensor(states).to(self.device)\n",
    "    actions_tensor = torch.LongTensor(actions).to(self.device)\n",
    "    rewards_tensor = torch.FloatTensor(rewards).to(self.device)\n",
    "    next_states_tensor = torch.FloatTensor(next_states).to(self.device)\n",
    "    dones_tensor = torch.FloatTensor(dones).to(self.device)\n",
    "    \n",
    "    # 2. Calculate current Q values (model output)\n",
    "    current_q_values = self.model(states_tensor)\n",
    "    current_q_values = current_q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # 3. Calculate target Q values (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        if self.use_double_dqn:\n",
    "            # Double DQN: use policy network to select action, target network to evaluate\n",
    "            next_actions = self.model(next_states_tensor).argmax(1)\n",
    "            next_q_values = self.target_model(next_states_tensor).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # Standard DQN: use target network for both selection and evaluation\n",
    "            next_q_values = self.target_model(next_states_tensor).max(1)[0]\n",
    "        \n",
    "        # Calculate target: r + gamma * max(Q(s', a')) * (1 - done)\n",
    "        target_q_values = rewards_tensor + self.gamma * next_q_values * (1 - dones_tensor)\n",
    "    \n",
    "    # 4. Calculate loss\n",
    "    loss = self.loss(current_q_values, target_q_values)\n",
    "    \n",
    "    # 5. Perform gradient descent\n",
    "    self.optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()             # Calculate gradients\n",
    "    self.optimizer.step()       # Update weights\n",
    "\n",
    "    self.nr_training_steps += 1\n",
    "    \n",
    "    # reduce epsilon\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e246cf-b487-4e93-acc9-1b18e3d1417e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b4f1b16af9ee8053a8839b775cab2e9",
     "grade": true,
     "grade_id": "cell-38fb10ebd1e39242",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test with a small version of the agent\n",
    "agent = DQNAgent(env.observation_space, env.action_space,\n",
    "                 model_cls=DQNetwork,\n",
    "                 device=device,\n",
    "                       gamma=0.99,\n",
    "                       epsilon=0.5,\n",
    "                       epsilon_min=0.03,\n",
    "                       epsilon_decay=0.9,\n",
    "                       learning_rate=0.0005,\n",
    "                       training_frequency=1,\n",
    "                       target_update_frequency=20,\n",
    "                       tau=0.00,\n",
    "                       use_double_dqn=False,\n",
    "                       batch_size=4,\n",
    "                       memory_size=16)\n",
    "obs, info = env.reset()\n",
    "action = agent.step_first(obs)\n",
    "\n",
    "for i in range(20):\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    action = agent.step(obs, reward, done)\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "        action = agent.step_first(obs)\n",
    "assert agent.nr_training_steps > 4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b96b12-e8dd-41f6-a8dc-6acd8c2f8223",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented a full DQN Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ef19c-2e2a-4d97-a173-34322b805f55",
   "metadata": {},
   "source": [
    "## Complete agent for training and evaluation.\n",
    "\n",
    "We will add some additional methods to the agent in order to train and evaluate it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b25676-7364-48ef-9ebb-9ac55e730bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, env: gym.Env, nr_episodes_to_train: int,  eval_env: gym.Env, eval_frequency: int, reset_options={}):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environment for the given number of steps.\n",
    "    Args:\n",
    "        env: The environment on which to train the agent\n",
    "        nr_episodes_to_train: the number of episodes to train\n",
    "        eval_env: Environment for evaluation\n",
    "        eval_frequency: Frequency of evaluation of the trained agent\n",
    "    \"\"\"\n",
    "    nr_episodes = 0\n",
    "    while True:\n",
    "        obs, _ = env.reset(options=reset_options)\n",
    "        a = self.step_first(obs)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            done = done or truncated\n",
    "            a = self.step(obs, reward, done)\n",
    "\n",
    "        nr_episodes += 1\n",
    "        if nr_episodes % eval_frequency == 0:\n",
    "            rewards = self.evaluate(eval_env, 10, reset_options=reset_options)\n",
    "            print(f'Evaluation: episode {nr_episodes}, epsilon: {self.epsilon} mean reward: {np.mean(rewards)}')\n",
    "\n",
    "        if nr_episodes >= nr_episodes_to_train:\n",
    "            return\n",
    "\n",
    "def evaluate(self, env: gym.Env, nr_episodes: int, reset_options={}):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment on which to evaluate the agent\n",
    "        nr_episodes: the number of episodes to evaluate\n",
    "    Returns:\n",
    "        the rewards for the episodes\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for e in range(nr_episodes):\n",
    "        obs, _ = env.reset(options=reset_options)\n",
    "        obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "        a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        # some environments do not support truncated episodes, so we additionally check for a maximal number of steps\n",
    "        while not done and not truncated:\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            obs_tensor = torch.from_numpy(obs).to(self.device)\n",
    "            a = self.calculate_action(obs_tensor, stochastic=False)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e050db9-1f7d-4290-9e6d-03c4a936fa69",
   "metadata": {},
   "source": [
    "## Example Training\n",
    "\n",
    "Here is an example with some hyperparameters and a short training time (that will not be enough to actually train the full agent). You can adjust the parameters and see if you get good results. But for handing in the exercise, put it back to a short training :-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732052e5-bc4f-40d4-a496-18babc377d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(environment_name)\n",
    "env_eval = gym.make(environment_name, render_mode='rgb_array')\n",
    "\n",
    " # Hyperparameters will be quite important here.\n",
    "q_agent = DQNAgent(env_train.observation_space, env_train.action_space,\n",
    "                   model_cls=DQNetwork,\n",
    "                   device=device,\n",
    "                   gamma=0.99,\n",
    "                   epsilon=0.9,\n",
    "                   epsilon_min=0.03,\n",
    "                   epsilon_decay=0.999,\n",
    "                   learning_rate=0.05,\n",
    "                   training_frequency=1,\n",
    "                   target_update_frequency=1,\n",
    "                   tau=0.01,\n",
    "                   use_double_dqn=False,\n",
    "                   batch_size=256,\n",
    "                   memory_size=10000)\n",
    "\n",
    "# the parameters are not optimal, but should already give a result\n",
    "q_agent.train(env_train, nr_episodes_to_train=1000, eval_env=env_eval, eval_frequency=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b3c3b-1d54-4e5a-8a07-cef082a51899",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.evaluate(env_eval, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7ac82-ea78-4f37-bb3d-86cc843a1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(environment_name, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d4871-50fa-4122-97a5-134798d52b52",
   "metadata": {},
   "source": [
    "# Bonus Exercise\n",
    "\n",
    "Can you try training for another example, like the mountain car from the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f787162-82ef-4a51-8e8e-d488863dfa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_env = 'MountainCar-v0'\n",
    "\n",
    "env_train = gym.make(other_env)\n",
    "env_eval = gym.make(other_env, render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation space: {env_eval.observation_space}')\n",
    "print(f'Action space: {env_eval.action_space}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8aaaaa-c62e-4169-9dbd-8d8bcc9b0350",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4623ab4dec6a1df0eeb4f6c804f37f13",
     "grade": false,
     "grade_id": "cell-a4cde5d226be73e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69d7ad-d39a-454b-baed-3afb680f8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with a few steps\n",
    "env = gym.make(other_env, render_mode='rgb_array')\n",
    "obs, _ = env.reset(options=dict(low=0.4, high=0.49))\n",
    "obs_tensor = torch.from_numpy(obs).to(device)\n",
    "for _ in range(200):\n",
    "    action = q_agent.calculate_action(obs_tensor, stochastic=False)\n",
    "    obs, _, done, _, _ = env.step(action)  # Take a random action\n",
    "    obs_tensor = torch.from_numpy(obs).to(device)\n",
    "    display_environment(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e40e83-197e-4c0d-8915-77e241053ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
