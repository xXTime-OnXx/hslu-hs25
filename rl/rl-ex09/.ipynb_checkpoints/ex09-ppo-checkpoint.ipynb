{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3dedc86f-1d78-46d5-a1af-08cac8f4855c",
   "metadata": {},
   "source": [
    "# State-of-the-art PPO algorithm\n",
    "\n",
    "In this exercise/tutorial we want to build a state-of-the-art PPO implementation using many of the recommended choices from various sources. For example\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "and\n",
    "https://arxiv.org/abs/2006.05990\n",
    "and sources from spinning up, baselines and other RL libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b131c1db58f5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.426580Z",
     "start_time": "2025-05-05T12:18:51.860656Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install jdc\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyglet\n",
    "import ipywidgets\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb358c684638ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.435047Z",
     "start_time": "2025-05-05T12:18:52.433462Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82905370e23437a0",
   "metadata": {},
   "source": [
    "## Actor and critic networks\n",
    "\n",
    "We will first implement the policy and actor networks. Some of the results found lead to the following implementation choices:\n",
    "- Seperate actor and critic networks often often in better performance than networks with shared weights\n",
    "- The width of the actor networks depends on the complexity of the task, if it is not wide enough (number of channels in intermediate layers) or too wide a performance drop occurs\n",
    "- The critic network can be wider and there does not seem to be a performance penalty if it is too wide.\n",
    "- Two hidden layers work well for most tasks\n",
    "- Tanh works best, ReLU works worst as activation function\n",
    "- An orthogonal initialization might improve performance, but seems not too important. However, the last layer should be initialized with lower values such as to cause action preference values (logits) of 0 in the beginning.\n",
    "\n",
    "We divide the methods into the ones used by rollouts and evalulation (get_action_and_value, get_value, get_action) which both need results in numpy and the one used for the training step (get_probs_and_value) which supply torch tensors and also return the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7817af5b081f6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.445361Z",
     "start_time": "2025-05-05T12:18:52.440506Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bde223bbae7820ca4566958c3aca368d",
     "grade": false,
     "grade_id": "cell-3a7b41f33f68fb28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Build the agent networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_obs, n_action):\n",
    "        super(AgentNetwork, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            # use the initialization function below for linear layers\n",
    "            self.init_linear(nn.Linear(n_obs, 256)),\n",
    "            nn.Tanh(),\n",
    "            self.init_linear(nn.Linear(256, 256)),\n",
    "            nn.Tanh(),\n",
    "            self.init_linear(nn.Linear(256, 1), std=1.0)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            self.init_linear(nn.Linear(n_obs, 64)),\n",
    "            nn.Tanh(),\n",
    "            self.init_linear(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            self.init_linear(nn.Linear(64, n_action), std=0.01)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def init_linear(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        nn.init.orthogonal_(layer.weight, std)\n",
    "        nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x).numpy(force=True)\n",
    "\n",
    "    def get_action(self, x):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "        return action.numpy(force=True)\n",
    "\n",
    "    def get_action_and_value(self, x):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "\n",
    "        return (action.numpy(force=True),\n",
    "                probs.log_prob(action).numpy(force=True),\n",
    "                self.critic(x).numpy(force=True))\n",
    "\n",
    "    def get_probs_and_value(self, x, action):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        return probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03709649e1cfb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.455995Z",
     "start_time": "2025-05-05T12:18:52.454514Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b901d6cf4ff021e377da21f5dbf44be",
     "grade": true,
     "grade_id": "cell-9958dcac57b53cc6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = AgentNetwork(4,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b680b6c0fbdaad9",
   "metadata": {},
   "source": [
    "## Vectorized environments\n",
    "We will use vectorized environments for the rollouts. With vectorized environments, multiple copies of an environment can be run in parallel. We will first use the same environment as in the last exercise for trying our algorithm and then switch to a new one for the final evaluation.\n",
    "\n",
    "Optimally the different environments should be distributed to different CPUs decreasing the wall-clock time to gather experience in comparison to using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5a069647fc8d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.468119Z",
     "start_time": "2025-05-05T12:18:52.461603Z"
    }
   },
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name)\n",
    "num_envs = 10\n",
    "envs = gym.make_vec(environment_name, num_envs=num_envs, vectorization_mode=gym.VectorizeMode.SYNC)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "print(f'single env: {obs.shape}')\n",
    "\n",
    "obss, _ = envs.reset()\n",
    "print(f'vec envs: {obss.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f08fe663c3d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.481672Z",
     "start_time": "2025-05-05T12:18:52.479625Z"
    }
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.shape, env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a360506b69942a",
   "metadata": {},
   "source": [
    "We can apply both observations directly to the agent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfc652a95eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.504431Z",
     "start_time": "2025-05-05T12:18:52.499263Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_network = AgentNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "print(agent_network.get_action(torch.tensor(obs)))\n",
    "print(agent_network.get_action(torch.tensor(obss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e4e46b634ff32",
   "metadata": {},
   "source": [
    "The vectorized actions can be used as input to the envs step function to get all results in parallel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fe7a57b03f8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.527434Z",
     "start_time": "2025-05-05T12:18:52.525019Z"
    }
   },
   "outputs": [],
   "source": [
    "obs, rewards, done, truncated, info = envs.step(envs.action_space.sample())\n",
    "print(obs, rewards, done, truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7fccfe4e44fae",
   "metadata": {},
   "source": [
    "Note that some of the environments might be done earlier than others. Dealing with this is facilitated by an auto_reset parameter in the setup which is set to Next-Step Mode by default. So any env that is done, will automatically reset and return the observation of the reset in the next step:\n",
    "\n",
    "![Auto Reset Modes](autoreset-modes.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860001d505036c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.552455Z",
     "start_time": "2025-05-05T12:18:52.545353Z"
    }
   },
   "outputs": [],
   "source": [
    "envs_test = gym.make_vec(environment_name, num_envs=2, vectorization_mode=gym.VectorizeMode.SYNC)\n",
    "envs_test.reset()\n",
    "for step in range(5):\n",
    "    obs, rewards, done, truncated, info = envs_test.step(envs_test.action_space.sample())\n",
    "    print(obs, rewards, done, truncated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231e0cfb076da1d",
   "metadata": {},
   "source": [
    "## Agent class\n",
    "\n",
    "We will organize the agent class slightly different to deal with the vectorized environments but start with the constructor as always. We add some parameters in the constructor for training.\n",
    "\n",
    "### Optimizer\n",
    "Adam is recommended as optimizer, while RMSProb actually performs similarly. The most important parameter here is the learning rate for the optimizer. Decaying the learning rate might increase the performance slightly but is of secondary importance, so we will leave it out.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec302270aa6af186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.565415Z",
     "start_time": "2025-05-05T12:18:52.561136Z"
    }
   },
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Implement PPO training algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 num_envs: int,\n",
    "                 agent_network_cls,\n",
    "                 device,\n",
    "                 gamma: float,\n",
    "                 learning_rate: float,\n",
    "                 rollout_length: int,\n",
    "                 nr_epochs: int,\n",
    "                 batch_size: int,\n",
    "                 use_gae: bool,\n",
    "                 gae_lambda: float,\n",
    "                 clip_coef: float,\n",
    "                 value_loss_coef: float,\n",
    "                 entropy_loss_coef: float,):\n",
    "        \"\"\"\n",
    "        Initialize the PPO algorithm and the parameters it uses.\n",
    "        Args:\n",
    "            observation_space: the (single) observation space of the environment.\n",
    "            action_space: the (single) action space of the environment.\n",
    "            num_envs: the number of (vectorized) environments.\n",
    "            agent_network_cls: the class that implements the actor and critic networks.\n",
    "            device: the device (cpu, cuda or mps) to use for training\n",
    "            gamma: The discount factor.\n",
    "            learning_rate: The learning rate.\n",
    "            rollout_length: The lengths of the rollouts\n",
    "            nr_epochs: The number of epochs to train for after each rollout.\n",
    "            batch_size: The (mini-batch) size for training.\n",
    "            use_gae: Use generalized advantage estimation (true) or n-step returns\n",
    "            gae_lambda: The lambda parameter for generalized advantage estimation.\n",
    "            clip_coef: The clipping coefficient of the PPO algorithm.\n",
    "            value_loss_coef: The scaling of the value loss in the loss function.\n",
    "            entropy_loss_coef: The scaling of the entropy loss in the loss function.\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.agent_network_cls = agent_network_cls\n",
    "        self.num_envs = num_envs\n",
    "        self.device = device\n",
    "\n",
    "        # hyperparameters for training\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rollout_length = rollout_length\n",
    "        self.nr_epochs = nr_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gae = use_gae\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_coef = clip_coef\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_loss_coef = entropy_loss_coef\n",
    "\n",
    "        # create the network\n",
    "        self.agent_network = self.agent_network_cls(observation_space.shape[0], action_space.n)\n",
    "        self.agent_network.to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.agent_network.parameters(), lr=self.learning_rate, betas=(0.9, 0.999), eps=1e-5)\n",
    "\n",
    "        # the rollout data will be kept as numpy arrays and just the mini batches will be moved to\n",
    "        # tensors on the device.\n",
    "        self.obs = np.zeros((self.rollout_length, self.num_envs) + self.observation_space.shape)\n",
    "        self.actions = np.zeros((self.rollout_length, self.num_envs) + self.action_space.shape)\n",
    "        self.log_probs = np.zeros((self.rollout_length, self.num_envs))\n",
    "        self.rewards = np.zeros((self.rollout_length, self.num_envs))\n",
    "        self.dones = np.zeros((self.rollout_length, self.num_envs))\n",
    "        self.values = np.zeros((self.rollout_length, self.num_envs))\n",
    "\n",
    "        # calculated returns and advantages\n",
    "        self.returns = np.zeros((self.rollout_length, self.num_envs))\n",
    "        self.advantages = np.zeros((self.rollout_length, self.num_envs))\n",
    "\n",
    "        # keeping track of overall number of steps\n",
    "        self.global_step = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697149747855d98",
   "metadata": {},
   "source": [
    "## Rollouts\n",
    "\n",
    "We first want to calculate the rollouts. We will give the next observation and dones as input to the rollout and also return them at the end of the rollouts. These make it easier to start the rollouts (with resetting the environments outside of the rollout) and to continue the next rollouts from where the last ones left off.\n",
    "So the given next_obs and next_dones should be saved for step 0, together with the action, log probability and value obtained from this observation and then the reward from the environment step. The observation and dones returned from the environment step are for step 1 and so on.\n",
    "The last observation and dones should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b9f2440a6baef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.582448Z",
     "start_time": "2025-05-05T12:18:52.580294Z"
    }
   },
   "outputs": [],
   "source": [
    "%%add_to PPOAgent\n",
    "def rollout(self, envs, next_obs, next_dones):\n",
    "    \"\"\"\n",
    "    Calculate the rollout for the vectorized environment.\n",
    "\n",
    "    Args:\n",
    "        envs: The environments for the rollout. The number of envs must correspond to the\n",
    "        number given in the constructor\n",
    "        next_obs: The next observations for the rollout, i.e. the observations for the first step.\n",
    "        next_dones: The next dones for the rollout.\n",
    "    Returns: the observations and dones to be used to start the next rollout.\n",
    "\n",
    "    \"\"\"\n",
    "    # do the rollouts for the number of steps\n",
    "    for step in range(0, self.rollout_length):\n",
    "        self.global_step += self.num_envs\n",
    "\n",
    "        self.obs[step] = next_obs\n",
    "        self.dones[step] = next_dones\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_obs_tensor = torch.tensor(next_obs).to(self.device)\n",
    "            action, log_prob, value = self.agent_network.get_action_and_value(next_obs_tensor)\n",
    "        self.values[step] = value.flatten()\n",
    "        self.actions[step] = action\n",
    "        self.log_probs[step] = log_prob\n",
    "\n",
    "        next_obs, reward, next_dones, _, _ = envs.step(action)\n",
    "        self.rewards[step] = reward\n",
    "\n",
    "    # return the next obs and dones as they will be used for the next rollout\n",
    "    return next_obs, next_dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f923072d8dd89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.672307Z",
     "start_time": "2025-05-05T12:18:52.587625Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agent = PPOAgent(env.observation_space, env.action_space,\n",
    "                     num_envs=num_envs,\n",
    "                     agent_network_cls=AgentNetwork,\n",
    "                     device=device,\n",
    "                     gamma=1.0,\n",
    "                     learning_rate=0.002,\n",
    "                     rollout_length=256,\n",
    "                     nr_epochs=5,\n",
    "                     batch_size=64,\n",
    "                     use_gae=False,\n",
    "                     gae_lambda=0.95,\n",
    "                     clip_coef=0.2,\n",
    "                     value_loss_coef=0.5,\n",
    "                     entropy_loss_coef=0.01)\n",
    "next_obs, _ = envs.reset()\n",
    "next_dones = np.zeros(num_envs)\n",
    "next_obs, next_dones = agent.rollout(envs, next_obs, next_dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9246ca93443c93d",
   "metadata": {},
   "source": [
    "## Calculate returns\n",
    "We need to calculate the returns (for the value approximation) and the advantage function (for the policy loss). We will first implement the n-step returns.\n",
    "\n",
    "When using rollouts the n actually depends on the position of the sample in the rollout. I.e. we just calculate backwards from the end of the rollout and bootstrap the calculation with the value function computed from the next observation. There can be resets within the rollouts, which are marked with the dones=True for these positions that have to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13748c7861cb4db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.686121Z",
     "start_time": "2025-05-05T12:18:52.683772Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6061d80df33351438fbbc81397e52a7",
     "grade": false,
     "grade_id": "cell-6fead21004634452",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to PPOAgent\n",
    "def calculate_returns(self, next_obs, next_dones):\n",
    "    \"\"\"\n",
    "    Calculate the returns and the advantages from the collected rollouts.\n",
    "    Args:\n",
    "        next_obs: the next observation at the end of the rollouts\n",
    "        next dones: the next dones at the end of the rollouts\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # get the value for the next observation to bootstrap the returns\n",
    "        next_obs_tensor = torch.tensor(next_obs).to(self.device)\n",
    "        next_value = self.agent_network.get_value(next_obs_tensor).reshape(1, -1)\n",
    "\n",
    "        # calculate the returns backwards from the rewards\n",
    "        for t in reversed(range(self.rollout_length)):\n",
    "            if t == self.rollout_length - 1:\n",
    "                next_is_non_terminal = 1.0 - next_dones\n",
    "                next_return = next_value\n",
    "            else:\n",
    "                next_is_non_terminal = 1.0 - self.dones[t + 1]\n",
    "                next_return = self.returns[t + 1]\n",
    "            # now calculate the returns (in self.returns) and the advantages (self.advantages) for step t\n",
    "            self.returns[t] = self.rewards[t] + self.gamma * next_is_non_terminal * next_return\n",
    "            self.advantages[t] = self.returns[t] - self.values[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3f632d02b0546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.697706Z",
     "start_time": "2025-05-05T12:18:52.694659Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "489e3930915472a875a5a977789f0263",
     "grade": true,
     "grade_id": "cell-b8932a271a538dd8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent.calculate_returns(next_obs, next_dones)\n",
    "print(agent.advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3243fb88f84385e",
   "metadata": {},
   "source": [
    "### Calculate Generalized Advantage Estimates\n",
    "\n",
    "The other, and better, possibility is to calculate the advantages using generalize advantage estimates. They can be calculated in reverse order of the rollout by scaling the last GAE value and adding the TD Error for the time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b2049e6b0763f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.726791Z",
     "start_time": "2025-05-05T12:18:52.724030Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fa43f810312325d4b7c2ae653e61352",
     "grade": false,
     "grade_id": "cell-efdbc2163c788616",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to PPOAgent\n",
    "\n",
    "def calculate_gae(self, next_obs, next_dones):\n",
    "    \"\"\"\n",
    "    Calculate the advantages from the rollouts using the GAE approach. This can be done\n",
    "    iteratively from the end by multiplying the previous advantage by the gamma and lambda\n",
    "    factors and adding the TD error.\n",
    "\n",
    "    The returns can then be calculated from the advantages and the value functions.\n",
    "    Args:\n",
    "        next_obs: the next observation at the end of the rollouts\n",
    "        next_dones: the next dones at the end of the rollouts\n",
    "    \"\"\"\n",
    "    with (torch.no_grad()):\n",
    "        # get the value for the next observation to bootstrap the returns\n",
    "        next_obs_tensor = torch.tensor(next_obs).to(self.device)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(self.rollout_length)):\n",
    "            if t == self.rollout_length - 1:\n",
    "                next_is_non_terminal = 1.0 - next_dones\n",
    "                next_value = self.agent_network.get_value(next_obs_tensor).reshape(1, -1)\n",
    "            else:\n",
    "                next_is_non_terminal = 1.0 - self.dones[t + 1]\n",
    "                next_value = self.values[t + 1]\n",
    "            # calculate TD error, advantage and returns. Save the alst GAE value in last_gae\n",
    "            delta = self.rewards[t] + self.gamma * next_is_non_terminal * next_value - self.values[t]\n",
    "            last_gae = delta + self.gamma * self.gae_lambda * next_is_non_terminal * last_gae\n",
    "            self.advantages[t] = last_gae\n",
    "            self.returns[t] = last_gae + self.values[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517e9c71ffa8668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.742480Z",
     "start_time": "2025-05-05T12:18:52.738732Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb7a7152e1e3dae049ae98774b505895",
     "grade": true,
     "grade_id": "cell-0dea28db6c37056f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent.calculate_gae(next_obs, next_dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100015a-3f86-430a-94e5-59cddc9af488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.749038Z",
     "start_time": "2025-05-05T12:18:52.747576Z"
    }
   },
   "source": [
    "## Train one epoch\n",
    "\n",
    "Now we want to train one epoch using the rollouts and the already calculated advantages and returns. The following steps are necessary:\n",
    "- Flatten the data from the vectorized environment so that they look like from one environment\n",
    "- Shuffle the indices\n",
    "- Calculate start and end of a minibatch and select the indices for the minibatch from this range in the shuffled indices\n",
    "- Calculate the tensors of the necessary values\n",
    "- Calculate the log probabilities, entropy and value function from the observation and actions in the minibatch\n",
    "- Calculate the ratio between the new and old policy (use the difference between the log probabilities and apply the exponential function to the result)\n",
    "- Calculate the PPO policy loss as follows:\n",
    "    - Calculate -advantage * ratio\n",
    "    - Calculate -advantage * clamp (ratio, 1-clip_coef, 1+clip_coef)\n",
    "    - Calculate the max of those two values\n",
    "    - Calculate the mean of the result\n",
    "- Calculate the value loss\n",
    "- Calculate the entropy loss\n",
    "- Calculate the total loss as sum of all losses\n",
    "- Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14453217-b6c2-4cb9-819c-e8e0f6e4808d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee821523a40e167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.756771Z",
     "start_time": "2025-05-05T12:18:52.753699Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e97c4cc8b3beea4500af24e0e065088",
     "grade": false,
     "grade_id": "cell-cddd7106c76338ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to PPOAgent\n",
    "def train_epoch(self, verbose=False):\n",
    "    \"\"\"\n",
    "    Train one epoch using the collected rollouts and calculated advantages. Looping over epochs needs to be done\n",
    "    in the main training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    # we have 2D arrays of observations etc by step and environment, which we now reshape\n",
    "    obs = self.obs.reshape((-1,) + self.observation_space.shape)\n",
    "    actions = self.actions.reshape((-1,) + self.action_space.shape)\n",
    "    log_probs = self.log_probs.reshape(-1)\n",
    "    returns = self.returns.reshape(-1)\n",
    "    advantages = self.advantages.reshape(-1)\n",
    "\n",
    "    # we do shuffling of the indices and use the complete batch data\n",
    "    indices = np.arange(self.rollout_length)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # calculate the start and end positions of the minibatches\n",
    "    for start in range(0, self.rollout_length, self.batch_size):\n",
    "        # in case the rollout length is not a multiple of the batch size\n",
    "        end = min(start + self.batch_size, self.rollout_length)\n",
    "        mini_batch_indices = indices[start:end]\n",
    "\n",
    "        # convert the minibatch data to tensors where needed:\n",
    "        obs_tensor = torch.tensor(obs[mini_batch_indices], dtype=torch.float32).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions[mini_batch_indices]).to(self.device)\n",
    "        advantages_tensor = torch.tensor(advantages[mini_batch_indices], dtype=torch.float32).to(self.device)\n",
    "        returns_tensor = torch.tensor(returns[mini_batch_indices]).to(self.device)\n",
    "        log_probs_tensor = torch.tensor(log_probs[mini_batch_indices]).to(self.device)\n",
    "\n",
    "        # calculate the log probs and values using the current weights\n",
    "        new_log_probs, entropy, new_value = self.agent_network.get_probs_and_value(obs_tensor, actions_tensor)\n",
    "\n",
    "        # calculate the ratio between the old and new probabilities\n",
    "        log_ratio = new_log_probs - log_probs_tensor\n",
    "        ratio = torch.exp(log_ratio)\n",
    "\n",
    "        # calculate the clipped PPO loss (negative, as we want to minimize)\n",
    "        policy_loss_unclipped = -advantages_tensor * ratio\n",
    "        policy_loss_clipped = -advantages_tensor * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n",
    "        policy_loss = torch.max(policy_loss_unclipped, policy_loss_clipped).mean()\n",
    "\n",
    "        # calculate the value loss and scaled value loss (multiply by value_loss_coef)\n",
    "        v_loss = ((new_value.squeeze() - returns_tensor) ** 2).mean()\n",
    "        v_loss_scaled = self.value_loss_coef * v_loss\n",
    "\n",
    "        # calculate the entropy loss\n",
    "        entropy_loss = -entropy.mean()\n",
    "        entropy_loss_scaled = self.entropy_loss_coef * entropy_loss\n",
    "\n",
    "        # use combined loss for gradient calculation\n",
    "        loss = policy_loss + v_loss_scaled + entropy_loss_scaled\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # clipping is recommended but the actual clipping value is not so important\n",
    "        nn.utils.clip_grad_norm_(self.agent_network.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # display some values if verbose, should best be done using wand or similar instead\n",
    "\n",
    "        if verbose:\n",
    "            print(f'p: {policy_loss:7.4} v: {v_loss_scaled:7.4} e:{entropy_loss_scaled:7.4}', end='\\r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382252b700597e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:18:52.771743Z",
     "start_time": "2025-05-05T12:18:52.762028Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d1385c7d1455d407f9e0b6bd5d28216",
     "grade": true,
     "grade_id": "cell-04e3845275d3678a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent.train_epoch(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3eb1daf0550a1",
   "metadata": {},
   "source": [
    "## Train loop and evaluation\n",
    "\n",
    "As the last task we now have to put it all together in the main training loop. For each train step\n",
    "- calculate the rollouts\n",
    "- calculate the advantages and returns (either by gae or returns)\n",
    "- train for the specified number of epochs\n",
    "- (Variation: Calculate advantages between epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb401a50c8d0a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:19:31.750743Z",
     "start_time": "2025-05-05T12:19:31.747751Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfd07f15d3d41f46ba0798bc1034445",
     "grade": false,
     "grade_id": "cell-4832aef6d573e600",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to PPOAgent\n",
    "def train(self, envs: gym.Env, nr_steps: int, eval_env: gym.Env, eval_frequency: int,\n",
    "          eval_episodes: int, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Train the agent on the given environments for the given number of steps. One step is\n",
    "    one rollout and a training batch for the number of epochs specified.\n",
    "    Args:\n",
    "        envs: The environments to train on. The number of environments must match the number\n",
    "        given in the constructor parameter.\n",
    "        nr_steps: The number of steps to train for.\n",
    "        eval_env: The environment to evaluate on.\n",
    "        eval_frequency: How often to run the evaluation.\n",
    "        eval_episodes: How many episodes to run the evaluation.\n",
    "        verbose: display training stats\n",
    "\n",
    "    \"\"\"\n",
    "    next_obs, _ = envs.reset()\n",
    "    next_dones = np.zeros(self.num_envs)\n",
    "\n",
    "    for steps in range(nr_steps):\n",
    "        # Perform rollout\n",
    "        next_obs, next_dones = self.rollout(envs, next_obs, next_dones)\n",
    "        \n",
    "        # Calculate advantages and returns\n",
    "        if self.use_gae:\n",
    "            self.calculate_gae(next_obs, next_dones)\n",
    "        else:\n",
    "            self.calculate_returns(next_obs, next_dones)\n",
    "        \n",
    "        # Train for multiple epochs\n",
    "        for epoch in range(self.nr_epochs):\n",
    "            self.train_epoch(verbose=verbose)\n",
    "        \n",
    "        if steps % eval_frequency == 0:\n",
    "            evaluated_returns = self.evaluate(eval_env, eval_episodes)\n",
    "            if verbose:\n",
    "                print('')\n",
    "            print(f'Step {steps} : Return {np.mean(evaluated_returns)}', )\n",
    "\n",
    "def evaluate(self, env, nr_episodes: int):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the given environment for the given number of episodes.\n",
    "    Args:\n",
    "        env: the environment to evaluate on.\n",
    "        nr_episodes: the number of episodes to run the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        the undiscounted returns\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for episode in range(nr_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "        a = self.agent_network.get_action(obs_tensor)\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated :\n",
    "            obs, reward, done, truncated, _ = env.step(a)\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "            a = self.agent_network.get_action(obs_tensor)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db61cd-0c40-453d-9db1-9f0afd17031f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b54ead6550ab63073235308e26c6cc72",
     "grade": true,
     "grade_id": "cell-b4982f825e81b403",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent.train(envs,1,env,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee3840df28249a",
   "metadata": {},
   "source": [
    "## Test the full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf868580164f6779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:20:03.612480Z",
     "start_time": "2025-05-05T12:19:35.235432Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_envs = 10\n",
    "\n",
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name)\n",
    "envs = gym.make_vec(environment_name, num_envs=num_envs, vectorization_mode=gym.VectorizeMode.SYNC)\n",
    "\n",
    "network = AgentNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "next_obs, _ = envs.reset()\n",
    "next_dones = np.zeros(num_envs)\n",
    "\n",
    "agent = PPOAgent(env.observation_space, env.action_space,\n",
    "                 num_envs=num_envs,\n",
    "                 agent_network_cls=AgentNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.003,\n",
    "                 rollout_length=256,\n",
    "                 nr_epochs=5,\n",
    "                 batch_size=64,\n",
    "                 use_gae=True,\n",
    "                 gae_lambda=0.95,\n",
    "                 clip_coef=0.2,\n",
    "                 value_loss_coef=0.5,\n",
    "                 entropy_loss_coef=0.01)\n",
    "\n",
    "training_steps = 300\n",
    "agent.train(envs, training_steps, env, 10, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd78b3b5b63597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:19:02.519544Z",
     "start_time": "2025-05-05T12:10:36.173048Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_environment(env):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis('off') \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "\n",
    "def play_env(env, agent_network, device):\n",
    "    obs, _ = env.reset()\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    action = agent_network.get_action(obs_tensor)\n",
    "    for i in range(501):\n",
    "        display_environment(env)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        if done:\n",
    "           break\n",
    "\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "        action = agent_network.get_action(obs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1c15678fb29d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:19:02.521395Z",
     "start_time": "2025-05-05T12:10:36.180973Z"
    }
   },
   "outputs": [],
   "source": [
    "env_play = gym.make(environment_name, render_mode='rgb_array')\n",
    "play_env(env_play, agent.agent_network, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e92252-e17e-46d1-8f58-b2668e22229d",
   "metadata": {},
   "source": [
    "## Lunar Lander\n",
    "\n",
    " Next we want to try a new and slightly more complicated environment: Lunar Lander\n",
    "\n",
    " Do the default parameters work? Can we solve the environment with PPO?\n",
    "\n",
    " The goal of lunar lander is to get a return of >200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac229909a039b96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:27:41.100330Z",
     "start_time": "2025-05-05T12:27:40.930798Z"
    }
   },
   "outputs": [],
   "source": [
    "num_envs = 10\n",
    "\n",
    "environment_name = 'LunarLander-v3'\n",
    "env = gym.make(environment_name)\n",
    "envs = gym.make_vec(environment_name, num_envs=num_envs, vectorization_mode=gym.VectorizeMode.SYNC)\n",
    "\n",
    "network = AgentNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "next_obs, _ = envs.reset()\n",
    "next_dones = np.zeros(num_envs)\n",
    "\n",
    "agent = PPOAgent(env.observation_space, env.action_space,\n",
    "                 num_envs=num_envs,\n",
    "                 agent_network_cls=AgentNetwork,\n",
    "                 device=device,\n",
    "                 gamma=0.99,\n",
    "                 learning_rate=0.0003,\n",
    "                 rollout_length=512,\n",
    "                 nr_epochs=2,\n",
    "                 batch_size=128,\n",
    "                 use_gae=True,\n",
    "                 gae_lambda=0.97,\n",
    "                 clip_coef=0.2,\n",
    "                 value_loss_coef=0.1,\n",
    "                 entropy_loss_coef=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99537dbdd14a978e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T12:28:38.015582Z",
     "start_time": "2025-05-05T12:28:36.176578Z"
    }
   },
   "outputs": [],
   "source": [
    "training_steps = 1000\n",
    "agent.train(envs, training_steps, env, 50, 2, False)\n",
    "torch.save(agent.agent_network, 'lunar_lander.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd1d66-9727-4b49-8409-1ada04208a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_play = gym.make(environment_name, render_mode='rgb_array')\n",
    "play_env(env_play, agent.agent_network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f44fb-2f52-4492-b2d3-036ea9635dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_network = torch.load('lunar_lander.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956a75f-2308-453b-b564-c0ffcbc3691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_env(env_play, loaded_network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a91198-cdcb-4506-85d7-18fe207d982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
